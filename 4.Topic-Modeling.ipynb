{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "## Introduction\n",
    "Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "In this notebook, we will be covering the steps on how to do Latent Dirichlet Allocation (LDA), which is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, you need to provide (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up.\n",
    "\n",
    "Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling - Attempt #1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abject</th>\n",
       "      <th>...</th>\n",
       "      <th>zee</th>\n",
       "      <th>zen</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "      <th>Ã©clair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ali</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>anthony</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bill</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>dave</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hasan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>jim</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>joe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>john</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>louis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mike</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ricky</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 7465 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaah  aaaaahhhhhhh  aaaaauuugghhhhhh  aaaahhhhh  aaah  aah  abc  \\\n",
       "ali           0             0                 0          0     0    0    1   \n",
       "anthony       0             0                 0          0     0    0    0   \n",
       "bill          1             0                 0          0     0    0    0   \n",
       "bo            0             1                 1          1     0    0    0   \n",
       "dave          0             0                 0          0     1    0    0   \n",
       "hasan         0             0                 0          0     0    0    0   \n",
       "jim           0             0                 0          0     0    0    0   \n",
       "joe           0             0                 0          0     0    0    0   \n",
       "john          0             0                 0          0     0    0    0   \n",
       "louis         0             0                 0          0     0    3    0   \n",
       "mike          0             0                 0          0     0    0    0   \n",
       "ricky         0             0                 0          0     0    0    0   \n",
       "\n",
       "         abcs  ability  abject  ...  zee  zen  zeppelin  zero  zillion  \\\n",
       "ali         0        0       0  ...    0    0         0     0        0   \n",
       "anthony     0        0       0  ...    0    0         0     0        0   \n",
       "bill        1        0       0  ...    0    0         0     1        1   \n",
       "bo          0        1       0  ...    0    0         0     1        0   \n",
       "dave        0        0       0  ...    0    0         0     0        0   \n",
       "hasan       0        0       0  ...    2    1         0     1        0   \n",
       "jim         0        0       0  ...    0    0         0     0        0   \n",
       "joe         0        0       0  ...    0    0         0     0        0   \n",
       "john        0        0       0  ...    0    0         0     0        0   \n",
       "louis       0        0       0  ...    0    0         0     2        0   \n",
       "mike        0        0       0  ...    0    0         2     1        0   \n",
       "ricky       0        1       1  ...    0    0         0     0        0   \n",
       "\n",
       "         zombie  zombies  zoning  zoo  Ã©clair  \n",
       "ali           1        0       0    0       0  \n",
       "anthony       0        0       0    0       0  \n",
       "bill          1        1       1    0       0  \n",
       "bo            0        0       0    0       0  \n",
       "dave          0        0       0    0       0  \n",
       "hasan         0        0       0    0       0  \n",
       "jim           0        0       0    0       0  \n",
       "joe           0        0       0    0       0  \n",
       "john          0        0       0    0       1  \n",
       "louis         0        0       0    0       0  \n",
       "mike          0        0       0    0       0  \n",
       "ricky         0        0       0    1       0  \n",
       "\n",
       "[12 rows x 7465 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ali</th>\n",
       "      <th>anthony</th>\n",
       "      <th>bill</th>\n",
       "      <th>bo</th>\n",
       "      <th>dave</th>\n",
       "      <th>hasan</th>\n",
       "      <th>jim</th>\n",
       "      <th>joe</th>\n",
       "      <th>john</th>\n",
       "      <th>louis</th>\n",
       "      <th>mike</th>\n",
       "      <th>ricky</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>aaaaah</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aaaaahhhhhhh</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aaaaauuugghhhhhh</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aaaahhhhh</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aaah</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ali  anthony  bill  bo  dave  hasan  jim  joe  john  louis  \\\n",
       "aaaaah              0        0     1   0     0      0    0    0     0      0   \n",
       "aaaaahhhhhhh        0        0     0   1     0      0    0    0     0      0   \n",
       "aaaaauuugghhhhhh    0        0     0   1     0      0    0    0     0      0   \n",
       "aaaahhhhh           0        0     0   1     0      0    0    0     0      0   \n",
       "aaah                0        0     0   0     1      0    0    0     0      0   \n",
       "\n",
       "                  mike  ricky  \n",
       "aaaaah               0      0  \n",
       "aaaaahhhhhhh         0      0  \n",
       "aaaaauuugghhhhhh     0      0  \n",
       "aaaahhhhh            0      0  \n",
       "aaah                 0      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:08:10,700 : INFO : using symmetric alpha at 0.5\n",
      "2020-01-30 20:08:10,740 : INFO : using symmetric eta at 0.5\n",
      "2020-01-30 20:08:10,750 : INFO : using serial LDA version on this node\n",
      "2020-01-30 20:08:11,462 : INFO : running online (multi-pass) LDA training, 2 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-01-30 20:08:12,686 : INFO : -9.390 per-word bound, 671.0 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:08:12,690 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-01-30 20:08:12,885 : INFO : topic #0 (0.500): 0.006*\"fucking\" + 0.005*\"shit\" + 0.005*\"hes\" + 0.005*\"didnt\" + 0.004*\"say\" + 0.004*\"theyre\" + 0.004*\"want\" + 0.004*\"day\" + 0.004*\"oh\" + 0.004*\"come\"\n",
      "2020-01-30 20:08:12,890 : INFO : topic #1 (0.500): 0.008*\"fucking\" + 0.006*\"fuck\" + 0.006*\"shit\" + 0.006*\"say\" + 0.005*\"going\" + 0.005*\"oh\" + 0.005*\"theyre\" + 0.004*\"good\" + 0.004*\"thing\" + 0.004*\"want\"\n",
      "2020-01-30 20:08:12,894 : INFO : topic diff=1.084558, rho=1.000000\n",
      "2020-01-30 20:08:13,721 : INFO : -8.049 per-word bound, 264.9 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:08:13,724 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-01-30 20:08:13,844 : INFO : topic #0 (0.500): 0.005*\"shit\" + 0.005*\"want\" + 0.004*\"hes\" + 0.004*\"fucking\" + 0.004*\"say\" + 0.004*\"oh\" + 0.004*\"didnt\" + 0.004*\"going\" + 0.004*\"love\" + 0.004*\"day\"\n",
      "2020-01-30 20:08:13,847 : INFO : topic #1 (0.500): 0.009*\"fucking\" + 0.007*\"fuck\" + 0.006*\"shit\" + 0.006*\"say\" + 0.005*\"theyre\" + 0.005*\"going\" + 0.005*\"thing\" + 0.005*\"oh\" + 0.005*\"good\" + 0.005*\"didnt\"\n",
      "2020-01-30 20:08:13,849 : INFO : topic diff=0.263240, rho=0.577350\n",
      "2020-01-30 20:08:14,437 : INFO : -7.971 per-word bound, 251.0 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:08:14,440 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-01-30 20:08:14,549 : INFO : topic #0 (0.500): 0.005*\"want\" + 0.004*\"oh\" + 0.004*\"going\" + 0.004*\"love\" + 0.004*\"say\" + 0.004*\"shit\" + 0.004*\"hes\" + 0.004*\"really\" + 0.004*\"says\" + 0.003*\"cause\"\n",
      "2020-01-30 20:08:14,553 : INFO : topic #1 (0.500): 0.009*\"fucking\" + 0.007*\"fuck\" + 0.006*\"shit\" + 0.006*\"say\" + 0.005*\"theyre\" + 0.005*\"thing\" + 0.005*\"going\" + 0.005*\"didnt\" + 0.005*\"hes\" + 0.005*\"went\"\n",
      "2020-01-30 20:08:14,556 : INFO : topic diff=0.203008, rho=0.500000\n",
      "2020-01-30 20:08:15,051 : INFO : -7.936 per-word bound, 244.9 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:08:15,054 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-01-30 20:08:15,139 : INFO : topic #0 (0.500): 0.005*\"want\" + 0.005*\"love\" + 0.004*\"oh\" + 0.004*\"going\" + 0.004*\"say\" + 0.004*\"really\" + 0.004*\"shit\" + 0.004*\"dad\" + 0.004*\"says\" + 0.004*\"hes\"\n",
      "2020-01-30 20:08:15,142 : INFO : topic #1 (0.500): 0.009*\"fucking\" + 0.007*\"fuck\" + 0.006*\"shit\" + 0.006*\"say\" + 0.005*\"theyre\" + 0.005*\"thing\" + 0.005*\"didnt\" + 0.005*\"hes\" + 0.005*\"going\" + 0.005*\"went\"\n",
      "2020-01-30 20:08:15,144 : INFO : topic diff=0.125864, rho=0.447214\n",
      "2020-01-30 20:08:15,616 : INFO : -7.922 per-word bound, 242.5 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:08:15,619 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-01-30 20:08:15,684 : INFO : topic #0 (0.500): 0.005*\"want\" + 0.005*\"love\" + 0.005*\"oh\" + 0.004*\"going\" + 0.004*\"say\" + 0.004*\"dad\" + 0.004*\"really\" + 0.004*\"says\" + 0.004*\"shit\" + 0.003*\"hes\"\n",
      "2020-01-30 20:08:15,687 : INFO : topic #1 (0.500): 0.010*\"fucking\" + 0.007*\"fuck\" + 0.007*\"shit\" + 0.006*\"say\" + 0.006*\"theyre\" + 0.005*\"thing\" + 0.005*\"didnt\" + 0.005*\"hes\" + 0.005*\"went\" + 0.005*\"going\"\n",
      "2020-01-30 20:08:15,690 : INFO : topic diff=0.079065, rho=0.408248\n",
      "2020-01-30 20:08:16,133 : INFO : -7.916 per-word bound, 241.5 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:08:16,135 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-01-30 20:08:16,199 : INFO : topic #0 (0.500): 0.005*\"want\" + 0.005*\"love\" + 0.005*\"oh\" + 0.004*\"going\" + 0.004*\"say\" + 0.004*\"dad\" + 0.004*\"really\" + 0.004*\"says\" + 0.003*\"shit\" + 0.003*\"hes\"\n",
      "2020-01-30 20:08:16,202 : INFO : topic #1 (0.500): 0.010*\"fucking\" + 0.007*\"fuck\" + 0.007*\"shit\" + 0.006*\"say\" + 0.006*\"theyre\" + 0.005*\"thing\" + 0.005*\"didnt\" + 0.005*\"hes\" + 0.005*\"went\" + 0.005*\"going\"\n",
      "2020-01-30 20:08:16,204 : INFO : topic diff=0.049780, rho=0.377964\n",
      "2020-01-30 20:08:16,665 : INFO : -7.913 per-word bound, 241.0 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:08:16,668 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-01-30 20:08:16,731 : INFO : topic #0 (0.500): 0.005*\"want\" + 0.005*\"love\" + 0.005*\"oh\" + 0.004*\"going\" + 0.004*\"say\" + 0.004*\"dad\" + 0.004*\"really\" + 0.004*\"says\" + 0.003*\"shit\" + 0.003*\"hes\"\n",
      "2020-01-30 20:08:16,733 : INFO : topic #1 (0.500): 0.010*\"fucking\" + 0.007*\"shit\" + 0.007*\"fuck\" + 0.006*\"say\" + 0.006*\"theyre\" + 0.005*\"thing\" + 0.005*\"didnt\" + 0.005*\"went\" + 0.005*\"hes\" + 0.005*\"day\"\n",
      "2020-01-30 20:08:16,735 : INFO : topic diff=0.031880, rho=0.353553\n",
      "2020-01-30 20:08:17,146 : INFO : -7.912 per-word bound, 240.8 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:08:17,148 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-01-30 20:08:17,219 : INFO : topic #0 (0.500): 0.005*\"want\" + 0.005*\"love\" + 0.005*\"oh\" + 0.004*\"going\" + 0.004*\"say\" + 0.004*\"dad\" + 0.004*\"really\" + 0.004*\"says\" + 0.003*\"shit\" + 0.003*\"good\"\n",
      "2020-01-30 20:08:17,222 : INFO : topic #1 (0.500): 0.010*\"fucking\" + 0.007*\"shit\" + 0.007*\"fuck\" + 0.006*\"say\" + 0.006*\"theyre\" + 0.005*\"thing\" + 0.005*\"didnt\" + 0.005*\"went\" + 0.005*\"hes\" + 0.005*\"day\"\n",
      "2020-01-30 20:08:17,224 : INFO : topic diff=0.020746, rho=0.333333\n",
      "2020-01-30 20:08:17,608 : INFO : -7.911 per-word bound, 240.7 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:08:17,610 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-01-30 20:08:17,684 : INFO : topic #0 (0.500): 0.005*\"want\" + 0.005*\"love\" + 0.005*\"oh\" + 0.005*\"going\" + 0.004*\"say\" + 0.004*\"dad\" + 0.004*\"really\" + 0.004*\"says\" + 0.003*\"shit\" + 0.003*\"good\"\n",
      "2020-01-30 20:08:17,687 : INFO : topic #1 (0.500): 0.010*\"fucking\" + 0.007*\"shit\" + 0.007*\"fuck\" + 0.006*\"theyre\" + 0.006*\"say\" + 0.005*\"thing\" + 0.005*\"didnt\" + 0.005*\"went\" + 0.005*\"hes\" + 0.005*\"day\"\n",
      "2020-01-30 20:08:17,689 : INFO : topic diff=0.013726, rho=0.316228\n",
      "2020-01-30 20:08:18,136 : INFO : -7.911 per-word bound, 240.7 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:08:18,139 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-01-30 20:08:18,201 : INFO : topic #0 (0.500): 0.005*\"want\" + 0.005*\"love\" + 0.005*\"oh\" + 0.005*\"going\" + 0.004*\"say\" + 0.004*\"dad\" + 0.004*\"really\" + 0.004*\"says\" + 0.003*\"good\" + 0.003*\"shit\"\n",
      "2020-01-30 20:08:18,203 : INFO : topic #1 (0.500): 0.010*\"fucking\" + 0.007*\"shit\" + 0.007*\"fuck\" + 0.006*\"theyre\" + 0.006*\"say\" + 0.005*\"thing\" + 0.005*\"didnt\" + 0.005*\"went\" + 0.005*\"hes\" + 0.005*\"day\"\n",
      "2020-01-30 20:08:18,205 : INFO : topic diff=0.009248, rho=0.301511\n",
      "2020-01-30 20:08:18,213 : INFO : topic #0 (0.500): 0.005*\"want\" + 0.005*\"love\" + 0.005*\"oh\" + 0.005*\"going\" + 0.004*\"say\" + 0.004*\"dad\" + 0.004*\"really\" + 0.004*\"says\" + 0.003*\"good\" + 0.003*\"shit\"\n",
      "2020-01-30 20:08:18,215 : INFO : topic #1 (0.500): 0.010*\"fucking\" + 0.007*\"shit\" + 0.007*\"fuck\" + 0.006*\"theyre\" + 0.006*\"say\" + 0.005*\"thing\" + 0.005*\"didnt\" + 0.005*\"went\" + 0.005*\"hes\" + 0.005*\"day\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"want\" + 0.005*\"love\" + 0.005*\"oh\" + 0.005*\"going\" + 0.004*\"say\" + 0.004*\"dad\" + 0.004*\"really\" + 0.004*\"says\" + 0.003*\"good\" + 0.003*\"shit\"'),\n",
       " (1,\n",
       "  '0.010*\"fucking\" + 0.007*\"shit\" + 0.007*\"fuck\" + 0.006*\"theyre\" + 0.006*\"say\" + 0.005*\"thing\" + 0.005*\"didnt\" + 0.005*\"went\" + 0.005*\"hes\" + 0.005*\"day\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:09:43,413 : INFO : using symmetric alpha at 0.3333333333333333\n",
      "2020-01-30 20:09:43,418 : INFO : using symmetric eta at 0.3333333333333333\n",
      "2020-01-30 20:09:43,426 : INFO : using serial LDA version on this node\n",
      "2020-01-30 20:09:43,442 : INFO : running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-01-30 20:09:44,685 : INFO : -9.569 per-word bound, 759.6 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:09:44,689 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-01-30 20:09:44,839 : INFO : topic #0 (0.333): 0.008*\"fucking\" + 0.006*\"shit\" + 0.006*\"fuck\" + 0.005*\"day\" + 0.005*\"theyre\" + 0.005*\"little\" + 0.005*\"cause\" + 0.005*\"thing\" + 0.004*\"want\" + 0.004*\"life\"\n",
      "2020-01-30 20:09:44,844 : INFO : topic #1 (0.333): 0.006*\"fucking\" + 0.006*\"say\" + 0.005*\"didnt\" + 0.005*\"oh\" + 0.005*\"shit\" + 0.005*\"want\" + 0.004*\"good\" + 0.004*\"thing\" + 0.004*\"fuck\" + 0.004*\"really\"\n",
      "2020-01-30 20:09:44,848 : INFO : topic #2 (0.333): 0.007*\"fucking\" + 0.006*\"say\" + 0.005*\"going\" + 0.005*\"oh\" + 0.005*\"shit\" + 0.005*\"theyre\" + 0.005*\"hes\" + 0.005*\"fuck\" + 0.005*\"theres\" + 0.004*\"didnt\"\n",
      "2020-01-30 20:09:44,852 : INFO : topic diff=1.278139, rho=1.000000\n",
      "2020-01-30 20:09:45,593 : INFO : -8.167 per-word bound, 287.4 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:09:45,596 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-01-30 20:09:45,711 : INFO : topic #0 (0.333): 0.009*\"fucking\" + 0.008*\"shit\" + 0.007*\"fuck\" + 0.006*\"theyre\" + 0.005*\"cause\" + 0.005*\"thing\" + 0.005*\"day\" + 0.005*\"life\" + 0.005*\"hes\" + 0.005*\"little\"\n",
      "2020-01-30 20:09:45,714 : INFO : topic #1 (0.333): 0.006*\"say\" + 0.006*\"fucking\" + 0.005*\"want\" + 0.005*\"going\" + 0.005*\"love\" + 0.005*\"didnt\" + 0.004*\"oh\" + 0.004*\"good\" + 0.004*\"little\" + 0.004*\"day\"\n",
      "2020-01-30 20:09:45,717 : INFO : topic #2 (0.333): 0.006*\"fucking\" + 0.006*\"oh\" + 0.005*\"shit\" + 0.005*\"say\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"really\" + 0.005*\"didnt\" + 0.004*\"hes\" + 0.004*\"theyre\"\n",
      "2020-01-30 20:09:45,722 : INFO : topic diff=0.510198, rho=0.577350\n",
      "2020-01-30 20:09:46,418 : INFO : -7.998 per-word bound, 255.7 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:09:46,421 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-01-30 20:09:46,537 : INFO : topic #0 (0.333): 0.010*\"fucking\" + 0.008*\"shit\" + 0.007*\"fuck\" + 0.006*\"theyre\" + 0.006*\"cause\" + 0.005*\"thing\" + 0.005*\"theres\" + 0.005*\"hes\" + 0.005*\"day\" + 0.005*\"life\"\n",
      "2020-01-30 20:09:46,541 : INFO : topic #1 (0.333): 0.007*\"say\" + 0.006*\"fucking\" + 0.005*\"love\" + 0.005*\"want\" + 0.005*\"going\" + 0.004*\"dad\" + 0.004*\"didnt\" + 0.004*\"oh\" + 0.004*\"day\" + 0.004*\"good\"\n",
      "2020-01-30 20:09:46,545 : INFO : topic #2 (0.333): 0.005*\"oh\" + 0.005*\"shit\" + 0.005*\"fucking\" + 0.005*\"cause\" + 0.005*\"didnt\" + 0.005*\"really\" + 0.004*\"going\" + 0.004*\"say\" + 0.004*\"says\" + 0.004*\"good\"\n",
      "2020-01-30 20:09:46,549 : INFO : topic diff=0.320629, rho=0.500000\n",
      "2020-01-30 20:09:47,215 : INFO : -7.940 per-word bound, 245.6 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:09:47,218 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-01-30 20:09:47,288 : INFO : topic #0 (0.333): 0.010*\"fucking\" + 0.008*\"shit\" + 0.007*\"fuck\" + 0.006*\"theyre\" + 0.006*\"cause\" + 0.006*\"thing\" + 0.005*\"theres\" + 0.005*\"hes\" + 0.005*\"life\" + 0.005*\"day\"\n",
      "2020-01-30 20:09:47,295 : INFO : topic #1 (0.333): 0.007*\"say\" + 0.005*\"fucking\" + 0.005*\"love\" + 0.005*\"want\" + 0.005*\"going\" + 0.005*\"dad\" + 0.004*\"did\" + 0.004*\"didnt\" + 0.004*\"day\" + 0.004*\"joke\"\n",
      "2020-01-30 20:09:47,299 : INFO : topic #2 (0.333): 0.006*\"shit\" + 0.005*\"oh\" + 0.005*\"didnt\" + 0.005*\"cause\" + 0.005*\"really\" + 0.004*\"says\" + 0.004*\"fucking\" + 0.004*\"good\" + 0.004*\"goes\" + 0.004*\"lot\"\n",
      "2020-01-30 20:09:47,302 : INFO : topic diff=0.202092, rho=0.447214\n",
      "2020-01-30 20:09:47,807 : INFO : -7.915 per-word bound, 241.4 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:09:47,809 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-01-30 20:09:47,880 : INFO : topic #0 (0.333): 0.011*\"fucking\" + 0.008*\"shit\" + 0.008*\"fuck\" + 0.006*\"theyre\" + 0.006*\"cause\" + 0.006*\"thing\" + 0.005*\"theres\" + 0.005*\"hes\" + 0.005*\"life\" + 0.005*\"going\"\n",
      "2020-01-30 20:09:47,883 : INFO : topic #1 (0.333): 0.007*\"say\" + 0.005*\"love\" + 0.005*\"fucking\" + 0.005*\"want\" + 0.005*\"going\" + 0.005*\"dad\" + 0.004*\"joke\" + 0.004*\"did\" + 0.004*\"day\" + 0.004*\"little\"\n",
      "2020-01-30 20:09:47,885 : INFO : topic #2 (0.333): 0.006*\"shit\" + 0.005*\"didnt\" + 0.005*\"oh\" + 0.005*\"cause\" + 0.005*\"really\" + 0.004*\"says\" + 0.004*\"good\" + 0.004*\"lot\" + 0.004*\"goes\" + 0.004*\"fuck\"\n",
      "2020-01-30 20:09:47,888 : INFO : topic diff=0.123233, rho=0.408248\n",
      "2020-01-30 20:09:48,286 : INFO : -7.906 per-word bound, 239.8 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:09:48,288 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-01-30 20:09:48,368 : INFO : topic #0 (0.333): 0.011*\"fucking\" + 0.008*\"shit\" + 0.008*\"fuck\" + 0.007*\"theyre\" + 0.006*\"cause\" + 0.006*\"thing\" + 0.005*\"theres\" + 0.005*\"hes\" + 0.005*\"going\" + 0.005*\"life\"\n",
      "2020-01-30 20:09:48,371 : INFO : topic #1 (0.333): 0.007*\"say\" + 0.005*\"love\" + 0.005*\"fucking\" + 0.005*\"want\" + 0.005*\"going\" + 0.005*\"dad\" + 0.004*\"joke\" + 0.004*\"did\" + 0.004*\"day\" + 0.004*\"little\"\n",
      "2020-01-30 20:09:48,373 : INFO : topic #2 (0.333): 0.006*\"shit\" + 0.005*\"didnt\" + 0.005*\"cause\" + 0.005*\"oh\" + 0.005*\"really\" + 0.004*\"says\" + 0.004*\"lot\" + 0.004*\"good\" + 0.004*\"goes\" + 0.004*\"fuck\"\n",
      "2020-01-30 20:09:48,376 : INFO : topic diff=0.077124, rho=0.377964\n",
      "2020-01-30 20:09:48,818 : INFO : -7.902 per-word bound, 239.2 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:09:48,819 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-01-30 20:09:48,887 : INFO : topic #0 (0.333): 0.011*\"fucking\" + 0.008*\"fuck\" + 0.008*\"shit\" + 0.007*\"theyre\" + 0.006*\"cause\" + 0.006*\"thing\" + 0.006*\"theres\" + 0.005*\"hes\" + 0.005*\"going\" + 0.005*\"life\"\n",
      "2020-01-30 20:09:48,890 : INFO : topic #1 (0.333): 0.007*\"say\" + 0.005*\"love\" + 0.005*\"want\" + 0.005*\"fucking\" + 0.005*\"going\" + 0.005*\"dad\" + 0.004*\"joke\" + 0.004*\"did\" + 0.004*\"day\" + 0.004*\"little\"\n",
      "2020-01-30 20:09:48,897 : INFO : topic #2 (0.333): 0.006*\"shit\" + 0.005*\"didnt\" + 0.005*\"cause\" + 0.005*\"oh\" + 0.005*\"really\" + 0.004*\"says\" + 0.004*\"lot\" + 0.004*\"good\" + 0.004*\"mean\" + 0.004*\"goes\"\n",
      "2020-01-30 20:09:48,902 : INFO : topic diff=0.049399, rho=0.353553\n",
      "2020-01-30 20:09:49,382 : INFO : -7.901 per-word bound, 238.9 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:09:49,385 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-01-30 20:09:49,442 : INFO : topic #0 (0.333): 0.011*\"fucking\" + 0.008*\"fuck\" + 0.008*\"shit\" + 0.007*\"theyre\" + 0.006*\"cause\" + 0.006*\"thing\" + 0.006*\"theres\" + 0.005*\"hes\" + 0.005*\"going\" + 0.005*\"life\"\n",
      "2020-01-30 20:09:49,445 : INFO : topic #1 (0.333): 0.007*\"say\" + 0.006*\"love\" + 0.005*\"want\" + 0.005*\"going\" + 0.005*\"fucking\" + 0.005*\"dad\" + 0.004*\"joke\" + 0.004*\"did\" + 0.004*\"day\" + 0.004*\"little\"\n",
      "2020-01-30 20:09:49,447 : INFO : topic #2 (0.333): 0.006*\"shit\" + 0.005*\"didnt\" + 0.005*\"cause\" + 0.005*\"oh\" + 0.005*\"really\" + 0.004*\"says\" + 0.004*\"lot\" + 0.004*\"good\" + 0.004*\"mean\" + 0.004*\"goes\"\n",
      "2020-01-30 20:09:49,450 : INFO : topic diff=0.032274, rho=0.333333\n",
      "2020-01-30 20:09:49,858 : INFO : -7.900 per-word bound, 238.8 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:09:49,860 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-01-30 20:09:49,905 : INFO : topic #0 (0.333): 0.011*\"fucking\" + 0.008*\"fuck\" + 0.008*\"shit\" + 0.007*\"theyre\" + 0.006*\"cause\" + 0.006*\"thing\" + 0.006*\"theres\" + 0.005*\"hes\" + 0.005*\"going\" + 0.005*\"life\"\n",
      "2020-01-30 20:09:49,906 : INFO : topic #1 (0.333): 0.007*\"say\" + 0.006*\"love\" + 0.005*\"want\" + 0.005*\"going\" + 0.005*\"fucking\" + 0.005*\"dad\" + 0.004*\"joke\" + 0.004*\"did\" + 0.004*\"day\" + 0.004*\"little\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:09:49,908 : INFO : topic #2 (0.333): 0.006*\"shit\" + 0.005*\"didnt\" + 0.005*\"cause\" + 0.005*\"oh\" + 0.005*\"really\" + 0.004*\"says\" + 0.004*\"lot\" + 0.004*\"good\" + 0.004*\"mean\" + 0.004*\"goes\"\n",
      "2020-01-30 20:09:49,910 : INFO : topic diff=0.021468, rho=0.316228\n",
      "2020-01-30 20:09:50,275 : INFO : -7.899 per-word bound, 238.8 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:09:50,276 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-01-30 20:09:50,330 : INFO : topic #0 (0.333): 0.011*\"fucking\" + 0.008*\"fuck\" + 0.008*\"shit\" + 0.007*\"theyre\" + 0.006*\"cause\" + 0.006*\"thing\" + 0.006*\"theres\" + 0.005*\"hes\" + 0.005*\"going\" + 0.005*\"life\"\n",
      "2020-01-30 20:09:50,333 : INFO : topic #1 (0.333): 0.007*\"say\" + 0.006*\"love\" + 0.005*\"want\" + 0.005*\"going\" + 0.005*\"fucking\" + 0.005*\"dad\" + 0.004*\"joke\" + 0.004*\"did\" + 0.004*\"day\" + 0.004*\"little\"\n",
      "2020-01-30 20:09:50,337 : INFO : topic #2 (0.333): 0.006*\"shit\" + 0.005*\"didnt\" + 0.005*\"cause\" + 0.005*\"oh\" + 0.005*\"really\" + 0.004*\"says\" + 0.004*\"lot\" + 0.004*\"good\" + 0.004*\"mean\" + 0.004*\"goes\"\n",
      "2020-01-30 20:09:50,339 : INFO : topic diff=0.014522, rho=0.301511\n",
      "2020-01-30 20:09:50,346 : INFO : topic #0 (0.333): 0.011*\"fucking\" + 0.008*\"fuck\" + 0.008*\"shit\" + 0.007*\"theyre\" + 0.006*\"cause\" + 0.006*\"thing\" + 0.006*\"theres\" + 0.005*\"hes\" + 0.005*\"going\" + 0.005*\"life\"\n",
      "2020-01-30 20:09:50,349 : INFO : topic #1 (0.333): 0.007*\"say\" + 0.006*\"love\" + 0.005*\"want\" + 0.005*\"going\" + 0.005*\"fucking\" + 0.005*\"dad\" + 0.004*\"joke\" + 0.004*\"did\" + 0.004*\"day\" + 0.004*\"little\"\n",
      "2020-01-30 20:09:50,351 : INFO : topic #2 (0.333): 0.006*\"shit\" + 0.005*\"didnt\" + 0.005*\"cause\" + 0.005*\"oh\" + 0.005*\"really\" + 0.004*\"says\" + 0.004*\"lot\" + 0.004*\"good\" + 0.004*\"mean\" + 0.004*\"goes\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"fucking\" + 0.008*\"fuck\" + 0.008*\"shit\" + 0.007*\"theyre\" + 0.006*\"cause\" + 0.006*\"thing\" + 0.006*\"theres\" + 0.005*\"hes\" + 0.005*\"going\" + 0.005*\"life\"'),\n",
       " (1,\n",
       "  '0.007*\"say\" + 0.006*\"love\" + 0.005*\"want\" + 0.005*\"going\" + 0.005*\"fucking\" + 0.005*\"dad\" + 0.004*\"joke\" + 0.004*\"did\" + 0.004*\"day\" + 0.004*\"little\"'),\n",
       " (2,\n",
       "  '0.006*\"shit\" + 0.005*\"didnt\" + 0.005*\"cause\" + 0.005*\"oh\" + 0.005*\"really\" + 0.004*\"says\" + 0.004*\"lot\" + 0.004*\"good\" + 0.004*\"mean\" + 0.004*\"goes\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:10:10,423 : INFO : using symmetric alpha at 0.25\n",
      "2020-01-30 20:10:10,428 : INFO : using symmetric eta at 0.25\n",
      "2020-01-30 20:10:10,436 : INFO : using serial LDA version on this node\n",
      "2020-01-30 20:10:10,456 : INFO : running online (multi-pass) LDA training, 4 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-01-30 20:10:11,983 : INFO : -9.778 per-word bound, 877.7 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:10:11,986 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-01-30 20:10:12,138 : INFO : topic #0 (0.250): 0.007*\"fucking\" + 0.006*\"fuck\" + 0.006*\"say\" + 0.005*\"shit\" + 0.005*\"want\" + 0.005*\"theyre\" + 0.004*\"love\" + 0.004*\"did\" + 0.004*\"day\" + 0.004*\"life\"\n",
      "2020-01-30 20:10:12,151 : INFO : topic #1 (0.250): 0.009*\"fucking\" + 0.006*\"fuck\" + 0.005*\"going\" + 0.005*\"theyre\" + 0.005*\"theres\" + 0.005*\"didnt\" + 0.005*\"say\" + 0.005*\"oh\" + 0.005*\"hes\" + 0.005*\"thing\"\n",
      "2020-01-30 20:10:12,157 : INFO : topic #2 (0.250): 0.008*\"shit\" + 0.006*\"fucking\" + 0.005*\"day\" + 0.005*\"oh\" + 0.005*\"say\" + 0.005*\"went\" + 0.005*\"good\" + 0.005*\"going\" + 0.004*\"didnt\" + 0.004*\"want\"\n",
      "2020-01-30 20:10:12,162 : INFO : topic #3 (0.250): 0.007*\"fucking\" + 0.005*\"fuck\" + 0.004*\"hes\" + 0.004*\"going\" + 0.004*\"shit\" + 0.004*\"say\" + 0.004*\"theyre\" + 0.004*\"want\" + 0.004*\"didnt\" + 0.004*\"goes\"\n",
      "2020-01-30 20:10:12,167 : INFO : topic diff=1.505627, rho=1.000000\n",
      "2020-01-30 20:10:13,009 : INFO : -8.280 per-word bound, 310.7 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:10:13,025 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-01-30 20:10:13,150 : INFO : topic #0 (0.250): 0.007*\"fucking\" + 0.006*\"want\" + 0.006*\"shit\" + 0.006*\"fuck\" + 0.006*\"say\" + 0.005*\"love\" + 0.005*\"did\" + 0.005*\"going\" + 0.004*\"hes\" + 0.004*\"guys\"\n",
      "2020-01-30 20:10:13,150 : INFO : topic #1 (0.250): 0.008*\"fucking\" + 0.006*\"fuck\" + 0.006*\"say\" + 0.006*\"theyre\" + 0.006*\"didnt\" + 0.005*\"shit\" + 0.005*\"thing\" + 0.005*\"hes\" + 0.005*\"theres\" + 0.005*\"little\"\n",
      "2020-01-30 20:10:13,150 : INFO : topic #2 (0.250): 0.007*\"fucking\" + 0.007*\"shit\" + 0.006*\"went\" + 0.006*\"oh\" + 0.006*\"day\" + 0.005*\"going\" + 0.005*\"cause\" + 0.004*\"fuck\" + 0.004*\"good\" + 0.004*\"thing\"\n",
      "2020-01-30 20:10:13,165 : INFO : topic #3 (0.250): 0.005*\"says\" + 0.005*\"cause\" + 0.005*\"goes\" + 0.004*\"really\" + 0.004*\"id\" + 0.004*\"mean\" + 0.004*\"say\" + 0.004*\"going\" + 0.004*\"fucking\" + 0.004*\"want\"\n",
      "2020-01-30 20:10:13,168 : INFO : topic diff=0.633287, rho=0.577350\n",
      "2020-01-30 20:10:13,718 : INFO : -8.043 per-word bound, 263.8 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:10:13,718 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-01-30 20:10:13,780 : INFO : topic #0 (0.250): 0.007*\"fucking\" + 0.007*\"shit\" + 0.007*\"want\" + 0.006*\"fuck\" + 0.006*\"love\" + 0.005*\"say\" + 0.005*\"going\" + 0.005*\"did\" + 0.004*\"man\" + 0.004*\"dad\"\n",
      "2020-01-30 20:10:13,780 : INFO : topic #1 (0.250): 0.008*\"fucking\" + 0.006*\"say\" + 0.006*\"theyre\" + 0.006*\"fuck\" + 0.006*\"thing\" + 0.005*\"didnt\" + 0.005*\"shit\" + 0.005*\"little\" + 0.005*\"theres\" + 0.005*\"hes\"\n",
      "2020-01-30 20:10:13,796 : INFO : topic #2 (0.250): 0.008*\"fucking\" + 0.007*\"went\" + 0.007*\"shit\" + 0.007*\"oh\" + 0.006*\"day\" + 0.005*\"going\" + 0.005*\"fuck\" + 0.005*\"cause\" + 0.005*\"theyre\" + 0.004*\"thing\"\n",
      "2020-01-30 20:10:13,796 : INFO : topic #3 (0.250): 0.007*\"says\" + 0.005*\"cause\" + 0.005*\"goes\" + 0.005*\"really\" + 0.005*\"id\" + 0.005*\"jenny\" + 0.005*\"mean\" + 0.004*\"point\" + 0.004*\"say\" + 0.004*\"going\"\n",
      "2020-01-30 20:10:13,796 : INFO : topic diff=0.414277, rho=0.500000\n",
      "2020-01-30 20:10:14,269 : INFO : -7.959 per-word bound, 248.8 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:10:14,269 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-01-30 20:10:14,331 : INFO : topic #0 (0.250): 0.007*\"fucking\" + 0.007*\"shit\" + 0.007*\"want\" + 0.006*\"fuck\" + 0.006*\"love\" + 0.005*\"going\" + 0.005*\"did\" + 0.005*\"say\" + 0.005*\"didnt\" + 0.005*\"man\"\n",
      "2020-01-30 20:10:14,331 : INFO : topic #1 (0.250): 0.008*\"fucking\" + 0.007*\"say\" + 0.006*\"theyre\" + 0.006*\"fuck\" + 0.006*\"thing\" + 0.005*\"didnt\" + 0.005*\"little\" + 0.005*\"shit\" + 0.005*\"cause\" + 0.005*\"theres\"\n",
      "2020-01-30 20:10:14,331 : INFO : topic #2 (0.250): 0.009*\"fucking\" + 0.007*\"went\" + 0.007*\"oh\" + 0.007*\"shit\" + 0.006*\"day\" + 0.005*\"going\" + 0.005*\"fuck\" + 0.005*\"cause\" + 0.005*\"theyre\" + 0.005*\"women\"\n",
      "2020-01-30 20:10:14,331 : INFO : topic #3 (0.250): 0.007*\"says\" + 0.006*\"cause\" + 0.005*\"goes\" + 0.005*\"really\" + 0.005*\"jenny\" + 0.005*\"id\" + 0.005*\"mean\" + 0.004*\"point\" + 0.004*\"say\" + 0.004*\"kind\"\n",
      "2020-01-30 20:10:14,331 : INFO : topic diff=0.263820, rho=0.447214\n",
      "2020-01-30 20:10:15,095 : INFO : -7.926 per-word bound, 243.2 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:10:15,095 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-01-30 20:10:15,233 : INFO : topic #0 (0.250): 0.008*\"fucking\" + 0.008*\"shit\" + 0.007*\"want\" + 0.006*\"fuck\" + 0.006*\"love\" + 0.005*\"going\" + 0.005*\"did\" + 0.005*\"didnt\" + 0.005*\"man\" + 0.005*\"hes\"\n",
      "2020-01-30 20:10:15,233 : INFO : topic #1 (0.250): 0.007*\"fucking\" + 0.007*\"say\" + 0.006*\"theyre\" + 0.006*\"fuck\" + 0.006*\"thing\" + 0.005*\"little\" + 0.005*\"didnt\" + 0.005*\"cause\" + 0.005*\"theres\" + 0.005*\"shit\"\n",
      "2020-01-30 20:10:15,248 : INFO : topic #2 (0.250): 0.009*\"fucking\" + 0.007*\"went\" + 0.007*\"oh\" + 0.007*\"day\" + 0.006*\"shit\" + 0.005*\"fuck\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"women\" + 0.005*\"theyre\"\n",
      "2020-01-30 20:10:15,248 : INFO : topic #3 (0.250): 0.008*\"says\" + 0.006*\"cause\" + 0.006*\"goes\" + 0.005*\"jenny\" + 0.005*\"really\" + 0.005*\"id\" + 0.005*\"mean\" + 0.005*\"point\" + 0.004*\"kind\" + 0.004*\"say\"\n",
      "2020-01-30 20:10:15,248 : INFO : topic diff=0.168268, rho=0.408248\n",
      "2020-01-30 20:10:16,265 : INFO : -7.913 per-word bound, 240.9 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:10:16,268 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-01-30 20:10:16,346 : INFO : topic #0 (0.250): 0.008*\"fucking\" + 0.008*\"shit\" + 0.007*\"want\" + 0.006*\"fuck\" + 0.006*\"love\" + 0.005*\"going\" + 0.005*\"did\" + 0.005*\"didnt\" + 0.005*\"man\" + 0.005*\"hes\"\n",
      "2020-01-30 20:10:16,361 : INFO : topic #1 (0.250): 0.007*\"fucking\" + 0.007*\"say\" + 0.006*\"theyre\" + 0.006*\"fuck\" + 0.006*\"thing\" + 0.005*\"little\" + 0.005*\"didnt\" + 0.005*\"cause\" + 0.005*\"theres\" + 0.005*\"day\"\n",
      "2020-01-30 20:10:16,361 : INFO : topic #2 (0.250): 0.009*\"fucking\" + 0.007*\"went\" + 0.007*\"oh\" + 0.007*\"day\" + 0.006*\"shit\" + 0.005*\"fuck\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"women\" + 0.005*\"theyre\"\n",
      "2020-01-30 20:10:16,361 : INFO : topic #3 (0.250): 0.008*\"says\" + 0.006*\"cause\" + 0.006*\"goes\" + 0.006*\"jenny\" + 0.005*\"really\" + 0.005*\"id\" + 0.005*\"mean\" + 0.005*\"point\" + 0.004*\"kind\" + 0.004*\"uh\"\n",
      "2020-01-30 20:10:16,361 : INFO : topic diff=0.108423, rho=0.377964\n",
      "2020-01-30 20:10:17,079 : INFO : -7.907 per-word bound, 240.0 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:10:17,079 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-01-30 20:10:17,173 : INFO : topic #0 (0.250): 0.008*\"fucking\" + 0.008*\"shit\" + 0.007*\"want\" + 0.006*\"fuck\" + 0.006*\"love\" + 0.005*\"going\" + 0.005*\"did\" + 0.005*\"didnt\" + 0.005*\"man\" + 0.005*\"hes\"\n",
      "2020-01-30 20:10:17,177 : INFO : topic #1 (0.250): 0.007*\"fucking\" + 0.007*\"say\" + 0.006*\"theyre\" + 0.006*\"fuck\" + 0.006*\"thing\" + 0.005*\"little\" + 0.005*\"cause\" + 0.005*\"didnt\" + 0.005*\"theres\" + 0.005*\"day\"\n",
      "2020-01-30 20:10:17,180 : INFO : topic #2 (0.250): 0.010*\"fucking\" + 0.008*\"went\" + 0.007*\"oh\" + 0.007*\"day\" + 0.006*\"shit\" + 0.006*\"fuck\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"women\" + 0.005*\"theyre\"\n",
      "2020-01-30 20:10:17,185 : INFO : topic #3 (0.250): 0.008*\"says\" + 0.006*\"cause\" + 0.006*\"jenny\" + 0.006*\"goes\" + 0.005*\"really\" + 0.005*\"id\" + 0.005*\"mean\" + 0.005*\"point\" + 0.004*\"kind\" + 0.004*\"uh\"\n",
      "2020-01-30 20:10:17,189 : INFO : topic diff=0.070827, rho=0.353553\n",
      "2020-01-30 20:10:17,755 : INFO : -7.904 per-word bound, 239.6 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:10:17,755 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-01-30 20:10:17,818 : INFO : topic #0 (0.250): 0.008*\"fucking\" + 0.008*\"shit\" + 0.007*\"want\" + 0.006*\"fuck\" + 0.006*\"love\" + 0.005*\"going\" + 0.005*\"didnt\" + 0.005*\"did\" + 0.005*\"man\" + 0.005*\"hes\"\n",
      "2020-01-30 20:10:17,818 : INFO : topic #1 (0.250): 0.007*\"fucking\" + 0.007*\"say\" + 0.006*\"theyre\" + 0.006*\"fuck\" + 0.006*\"thing\" + 0.005*\"little\" + 0.005*\"cause\" + 0.005*\"didnt\" + 0.005*\"day\" + 0.005*\"theres\"\n",
      "2020-01-30 20:10:17,833 : INFO : topic #2 (0.250): 0.010*\"fucking\" + 0.008*\"went\" + 0.007*\"oh\" + 0.007*\"day\" + 0.006*\"shit\" + 0.006*\"fuck\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"women\" + 0.005*\"theyre\"\n",
      "2020-01-30 20:10:17,833 : INFO : topic #3 (0.250): 0.008*\"says\" + 0.006*\"cause\" + 0.006*\"jenny\" + 0.006*\"goes\" + 0.005*\"really\" + 0.005*\"id\" + 0.005*\"mean\" + 0.005*\"point\" + 0.004*\"kind\" + 0.004*\"uh\"\n",
      "2020-01-30 20:10:17,833 : INFO : topic diff=0.046938, rho=0.333333\n",
      "2020-01-30 20:10:18,402 : INFO : -7.903 per-word bound, 239.3 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:10:18,402 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-01-30 20:10:18,464 : INFO : topic #0 (0.250): 0.008*\"fucking\" + 0.008*\"shit\" + 0.007*\"want\" + 0.006*\"fuck\" + 0.006*\"love\" + 0.005*\"going\" + 0.005*\"didnt\" + 0.005*\"did\" + 0.005*\"man\" + 0.005*\"hes\"\n",
      "2020-01-30 20:10:18,480 : INFO : topic #1 (0.250): 0.007*\"fucking\" + 0.007*\"say\" + 0.006*\"theyre\" + 0.006*\"fuck\" + 0.006*\"thing\" + 0.005*\"little\" + 0.005*\"cause\" + 0.005*\"didnt\" + 0.005*\"day\" + 0.005*\"theres\"\n",
      "2020-01-30 20:10:18,480 : INFO : topic #2 (0.250): 0.010*\"fucking\" + 0.008*\"went\" + 0.007*\"oh\" + 0.007*\"day\" + 0.006*\"shit\" + 0.006*\"fuck\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"women\" + 0.005*\"theyre\"\n",
      "2020-01-30 20:10:18,480 : INFO : topic #3 (0.250): 0.008*\"says\" + 0.006*\"cause\" + 0.006*\"jenny\" + 0.006*\"goes\" + 0.005*\"really\" + 0.005*\"id\" + 0.005*\"mean\" + 0.005*\"point\" + 0.004*\"kind\" + 0.004*\"uh\"\n",
      "2020-01-30 20:10:18,480 : INFO : topic diff=0.031552, rho=0.316228\n",
      "2020-01-30 20:10:18,996 : INFO : -7.902 per-word bound, 239.2 perplexity estimate based on a held-out corpus of 12 documents with 41483 words\n",
      "2020-01-30 20:10:18,996 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-01-30 20:10:19,059 : INFO : topic #0 (0.250): 0.008*\"fucking\" + 0.008*\"shit\" + 0.007*\"want\" + 0.006*\"fuck\" + 0.006*\"love\" + 0.005*\"going\" + 0.005*\"didnt\" + 0.005*\"did\" + 0.005*\"man\" + 0.005*\"hes\"\n",
      "2020-01-30 20:10:19,059 : INFO : topic #1 (0.250): 0.007*\"fucking\" + 0.007*\"say\" + 0.006*\"theyre\" + 0.006*\"fuck\" + 0.006*\"thing\" + 0.005*\"little\" + 0.005*\"cause\" + 0.005*\"didnt\" + 0.005*\"day\" + 0.005*\"theres\"\n",
      "2020-01-30 20:10:19,059 : INFO : topic #2 (0.250): 0.010*\"fucking\" + 0.008*\"went\" + 0.007*\"oh\" + 0.007*\"day\" + 0.006*\"shit\" + 0.006*\"fuck\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"women\" + 0.005*\"theyre\"\n",
      "2020-01-30 20:10:19,074 : INFO : topic #3 (0.250): 0.008*\"says\" + 0.006*\"cause\" + 0.006*\"jenny\" + 0.006*\"goes\" + 0.005*\"really\" + 0.005*\"id\" + 0.005*\"mean\" + 0.005*\"point\" + 0.004*\"kind\" + 0.004*\"uh\"\n",
      "2020-01-30 20:10:19,074 : INFO : topic diff=0.021509, rho=0.301511\n",
      "2020-01-30 20:10:19,074 : INFO : topic #0 (0.250): 0.008*\"fucking\" + 0.008*\"shit\" + 0.007*\"want\" + 0.006*\"fuck\" + 0.006*\"love\" + 0.005*\"going\" + 0.005*\"didnt\" + 0.005*\"did\" + 0.005*\"man\" + 0.005*\"hes\"\n",
      "2020-01-30 20:10:19,090 : INFO : topic #1 (0.250): 0.007*\"fucking\" + 0.007*\"say\" + 0.006*\"theyre\" + 0.006*\"fuck\" + 0.006*\"thing\" + 0.005*\"little\" + 0.005*\"cause\" + 0.005*\"didnt\" + 0.005*\"day\" + 0.005*\"theres\"\n",
      "2020-01-30 20:10:19,090 : INFO : topic #2 (0.250): 0.010*\"fucking\" + 0.008*\"went\" + 0.007*\"oh\" + 0.007*\"day\" + 0.006*\"shit\" + 0.006*\"fuck\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"women\" + 0.005*\"theyre\"\n",
      "2020-01-30 20:10:19,090 : INFO : topic #3 (0.250): 0.008*\"says\" + 0.006*\"cause\" + 0.006*\"jenny\" + 0.006*\"goes\" + 0.005*\"really\" + 0.005*\"id\" + 0.005*\"mean\" + 0.005*\"point\" + 0.004*\"kind\" + 0.004*\"uh\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"fucking\" + 0.008*\"shit\" + 0.007*\"want\" + 0.006*\"fuck\" + 0.006*\"love\" + 0.005*\"going\" + 0.005*\"didnt\" + 0.005*\"did\" + 0.005*\"man\" + 0.005*\"hes\"'),\n",
       " (1,\n",
       "  '0.007*\"fucking\" + 0.007*\"say\" + 0.006*\"theyre\" + 0.006*\"fuck\" + 0.006*\"thing\" + 0.005*\"little\" + 0.005*\"cause\" + 0.005*\"didnt\" + 0.005*\"day\" + 0.005*\"theres\"'),\n",
       " (2,\n",
       "  '0.010*\"fucking\" + 0.008*\"went\" + 0.007*\"oh\" + 0.007*\"day\" + 0.006*\"shit\" + 0.006*\"fuck\" + 0.005*\"going\" + 0.005*\"cause\" + 0.005*\"women\" + 0.005*\"theyre\"'),\n",
       " (3,\n",
       "  '0.008*\"says\" + 0.006*\"cause\" + 0.006*\"jenny\" + 0.006*\"goes\" + 0.005*\"really\" + 0.005*\"id\" + 0.005*\"mean\" + 0.005*\"point\" + 0.004*\"kind\" + 0.004*\"uh\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling - Attempt #2 (Nouns Only)\n",
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). Check out the UPenn tag set: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ali</td>\n",
       "      <td>ladies and gentlemen please welcome to the sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>anthony</td>\n",
       "      <td>thank you thank you thank you san francisco th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bill</td>\n",
       "      <td>all right thank you thank you very much thank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bo</td>\n",
       "      <td>bo what old macdonald had a farm e i e i o and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>dave</td>\n",
       "      <td>this is dave he tells dirty jokes for a living...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hasan</td>\n",
       "      <td>whats up davis whats up im home i had to bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>jim</td>\n",
       "      <td>ladies and gentlemen please welcome to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>joe</td>\n",
       "      <td>ladies and gentlemen welcome joe rogan  wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>john</td>\n",
       "      <td>all right petunia wish me luck out there you w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>louis</td>\n",
       "      <td>introfade the music out lets roll hold there l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mike</td>\n",
       "      <td>wow hey thank you thanks thank you guys hey se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ricky</td>\n",
       "      <td>hello hello how you doing great thank you wow ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies and gentlemen please welcome to the sta...\n",
       "anthony  thank you thank you thank you san francisco th...\n",
       "bill      all right thank you thank you very much thank...\n",
       "bo       bo what old macdonald had a farm e i e i o and...\n",
       "dave     this is dave he tells dirty jokes for a living...\n",
       "hasan      whats up davis whats up im home i had to bri...\n",
       "jim         ladies and gentlemen please welcome to the ...\n",
       "joe         ladies and gentlemen welcome joe rogan  wha...\n",
       "john     all right petunia wish me luck out there you w...\n",
       "louis    introfade the music out lets roll hold there l...\n",
       "mike     wow hey thank you thanks thank you guys hey se...\n",
       "ricky    hello hello how you doing great thank you wow ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ali</td>\n",
       "      <td>ladies gentlemen stage ali hi thank hello na s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>anthony</td>\n",
       "      <td>thank thank people i em i francisco city world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bill</td>\n",
       "      <td>thank thank pleasure georgia area oasis i june...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bo</td>\n",
       "      <td>macdonald farm e i o farm pig e i i snort macd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>dave</td>\n",
       "      <td>jokes living stare work profound train thought...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hasan</td>\n",
       "      <td>whats davis whats home i netflix la york i son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>jim</td>\n",
       "      <td>ladies gentlemen stage mr jim jefferies thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>joe</td>\n",
       "      <td>ladies gentlemen joe fuck thanks phone fuckfac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>john</td>\n",
       "      <td>petunia thats hello hello chicago thank crowd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>louis</td>\n",
       "      <td>music lets lights lights thank i i place place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mike</td>\n",
       "      <td>wow hey thanks look insane years everyone i id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ricky</td>\n",
       "      <td>hello thank fuck thank im gon youre weve money...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies gentlemen stage ali hi thank hello na s...\n",
       "anthony  thank thank people i em i francisco city world...\n",
       "bill     thank thank pleasure georgia area oasis i june...\n",
       "bo       macdonald farm e i o farm pig e i i snort macd...\n",
       "dave     jokes living stare work profound train thought...\n",
       "hasan    whats davis whats home i netflix la york i son...\n",
       "jim      ladies gentlemen stage mr jim jefferies thank ...\n",
       "joe      ladies gentlemen joe fuck thanks phone fuckfac...\n",
       "john     petunia thats hello hello chicago thank crowd ...\n",
       "louis    music lets lights lights thank i i place place...\n",
       "mike     wow hey thanks look insane years everyone i id...\n",
       "ricky    hello thank fuck thank im gon youre weve money..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abortions</th>\n",
       "      <th>abuse</th>\n",
       "      <th>...</th>\n",
       "      <th>yummy</th>\n",
       "      <th>ze</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zee</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoo</th>\n",
       "      <th>Ã©clair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ali</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>anthony</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bill</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bo</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>dave</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hasan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>jim</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>joe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>john</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>louis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mike</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ricky</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 4633 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaahhhhhhh  aaaaauuugghhhhhh  aaaahhhhh  aah  abc  abcs  ability  \\\n",
       "ali                 0                 0          0    0    1     0        0   \n",
       "anthony             0                 0          0    0    0     0        0   \n",
       "bill                0                 0          0    0    0     1        0   \n",
       "bo                  1                 1          1    0    0     0        1   \n",
       "dave                0                 0          0    0    0     0        0   \n",
       "hasan               0                 0          0    0    0     0        0   \n",
       "jim                 0                 0          0    0    0     0        0   \n",
       "joe                 0                 0          0    0    0     0        0   \n",
       "john                0                 0          0    0    0     0        0   \n",
       "louis               0                 0          0    3    0     0        0   \n",
       "mike                0                 0          0    0    0     0        0   \n",
       "ricky               0                 0          0    0    0     0        1   \n",
       "\n",
       "         abortion  abortions  abuse  ...  yummy  ze  zealand  zee  zeppelin  \\\n",
       "ali             0          0      0  ...      0   0        0    0         0   \n",
       "anthony         2          0      0  ...      0   0       10    0         0   \n",
       "bill            0          0      0  ...      0   1        0    0         0   \n",
       "bo              0          0      0  ...      0   0        0    0         0   \n",
       "dave            0          1      0  ...      0   0        0    0         0   \n",
       "hasan           0          0      0  ...      0   0        0    1         0   \n",
       "jim             0          0      0  ...      0   0        0    0         0   \n",
       "joe             0          0      1  ...      0   0        0    0         0   \n",
       "john            0          0      0  ...      0   0        0    0         0   \n",
       "louis           0          0      0  ...      0   0        0    0         0   \n",
       "mike            0          0      0  ...      0   0        0    0         2   \n",
       "ricky           0          0      0  ...      1   0        0    0         0   \n",
       "\n",
       "         zillion  zombie  zombies  zoo  Ã©clair  \n",
       "ali            0       1        0    0       0  \n",
       "anthony        0       0        0    0       0  \n",
       "bill           1       1        1    0       0  \n",
       "bo             0       0        0    0       0  \n",
       "dave           0       0        0    0       0  \n",
       "hasan          0       0        0    0       0  \n",
       "jim            0       0        0    0       0  \n",
       "joe            0       0        0    0       0  \n",
       "john           0       0        0    0       1  \n",
       "louis          0       0        0    0       0  \n",
       "mike           0       0        0    0       0  \n",
       "ricky          0       0        0    1       0  \n",
       "\n",
       "[12 rows x 4633 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:24:24,274 : INFO : using symmetric alpha at 0.5\n",
      "2020-01-30 20:24:24,279 : INFO : using symmetric eta at 0.5\n",
      "2020-01-30 20:24:24,286 : INFO : using serial LDA version on this node\n",
      "2020-01-30 20:24:24,295 : INFO : running online (multi-pass) LDA training, 2 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-01-30 20:24:25,128 : INFO : -8.942 per-word bound, 491.7 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:24:25,128 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-01-30 20:24:25,275 : INFO : topic #0 (0.500): 0.010*\"thing\" + 0.008*\"day\" + 0.007*\"man\" + 0.007*\"hes\" + 0.007*\"shit\" + 0.006*\"life\" + 0.006*\"gon\" + 0.006*\"way\" + 0.005*\"fuck\" + 0.005*\"kids\"\n",
      "2020-01-30 20:24:25,279 : INFO : topic #1 (0.500): 0.008*\"day\" + 0.008*\"shit\" + 0.008*\"life\" + 0.008*\"cause\" + 0.007*\"man\" + 0.007*\"way\" + 0.007*\"hes\" + 0.007*\"thing\" + 0.006*\"fuck\" + 0.006*\"guy\"\n",
      "2020-01-30 20:24:25,283 : INFO : topic diff=0.945844, rho=1.000000\n",
      "2020-01-30 20:24:25,801 : INFO : -7.756 per-word bound, 216.2 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:24:25,801 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-01-30 20:24:25,927 : INFO : topic #0 (0.500): 0.011*\"thing\" + 0.008*\"day\" + 0.007*\"man\" + 0.007*\"life\" + 0.007*\"hes\" + 0.006*\"fuck\" + 0.006*\"shit\" + 0.006*\"kids\" + 0.006*\"gon\" + 0.006*\"way\"\n",
      "2020-01-30 20:24:25,931 : INFO : topic #1 (0.500): 0.009*\"shit\" + 0.008*\"day\" + 0.007*\"life\" + 0.007*\"man\" + 0.007*\"cause\" + 0.007*\"way\" + 0.007*\"hes\" + 0.007*\"thing\" + 0.006*\"lot\" + 0.006*\"guy\"\n",
      "2020-01-30 20:24:25,936 : INFO : topic diff=0.277710, rho=0.577350\n",
      "2020-01-30 20:24:26,332 : INFO : -7.674 per-word bound, 204.3 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:24:26,336 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-01-30 20:24:26,432 : INFO : topic #0 (0.500): 0.011*\"thing\" + 0.009*\"day\" + 0.008*\"life\" + 0.007*\"man\" + 0.007*\"hes\" + 0.007*\"fuck\" + 0.006*\"kids\" + 0.006*\"cause\" + 0.006*\"way\" + 0.006*\"shit\"\n",
      "2020-01-30 20:24:26,436 : INFO : topic #1 (0.500): 0.009*\"shit\" + 0.008*\"day\" + 0.007*\"man\" + 0.007*\"life\" + 0.007*\"way\" + 0.006*\"hes\" + 0.006*\"thing\" + 0.006*\"cause\" + 0.006*\"lot\" + 0.006*\"guy\"\n",
      "2020-01-30 20:24:26,438 : INFO : topic diff=0.206088, rho=0.500000\n",
      "2020-01-30 20:24:26,801 : INFO : -7.634 per-word bound, 198.6 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:24:26,803 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-01-30 20:24:26,875 : INFO : topic #0 (0.500): 0.011*\"thing\" + 0.009*\"day\" + 0.008*\"life\" + 0.007*\"man\" + 0.007*\"hes\" + 0.007*\"cause\" + 0.007*\"fuck\" + 0.006*\"kids\" + 0.006*\"way\" + 0.006*\"things\"\n",
      "2020-01-30 20:24:26,880 : INFO : topic #1 (0.500): 0.009*\"shit\" + 0.008*\"day\" + 0.007*\"man\" + 0.006*\"way\" + 0.006*\"hes\" + 0.006*\"life\" + 0.006*\"thing\" + 0.006*\"lot\" + 0.006*\"cause\" + 0.006*\"guy\"\n",
      "2020-01-30 20:24:26,882 : INFO : topic diff=0.129369, rho=0.447214\n",
      "2020-01-30 20:24:27,226 : INFO : -7.616 per-word bound, 196.2 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:24:27,229 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-01-30 20:24:27,294 : INFO : topic #0 (0.500): 0.011*\"thing\" + 0.009*\"day\" + 0.008*\"life\" + 0.007*\"man\" + 0.007*\"hes\" + 0.007*\"cause\" + 0.007*\"fuck\" + 0.006*\"way\" + 0.006*\"kids\" + 0.006*\"things\"\n",
      "2020-01-30 20:24:27,297 : INFO : topic #1 (0.500): 0.009*\"shit\" + 0.007*\"day\" + 0.007*\"man\" + 0.006*\"hes\" + 0.006*\"way\" + 0.006*\"life\" + 0.006*\"thing\" + 0.006*\"lot\" + 0.006*\"guy\" + 0.006*\"dad\"\n",
      "2020-01-30 20:24:27,299 : INFO : topic diff=0.085283, rho=0.408248\n",
      "2020-01-30 20:24:27,601 : INFO : -7.607 per-word bound, 195.0 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:24:27,603 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-01-30 20:24:27,663 : INFO : topic #0 (0.500): 0.011*\"thing\" + 0.009*\"day\" + 0.009*\"life\" + 0.007*\"man\" + 0.007*\"hes\" + 0.007*\"cause\" + 0.007*\"fuck\" + 0.007*\"way\" + 0.006*\"kids\" + 0.006*\"things\"\n",
      "2020-01-30 20:24:27,666 : INFO : topic #1 (0.500): 0.009*\"shit\" + 0.007*\"day\" + 0.007*\"man\" + 0.006*\"hes\" + 0.006*\"way\" + 0.006*\"thing\" + 0.006*\"lot\" + 0.006*\"life\" + 0.006*\"guy\" + 0.006*\"fuck\"\n",
      "2020-01-30 20:24:27,669 : INFO : topic diff=0.053785, rho=0.377964\n",
      "2020-01-30 20:24:27,977 : INFO : -7.603 per-word bound, 194.4 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:24:27,979 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-01-30 20:24:28,048 : INFO : topic #0 (0.500): 0.011*\"thing\" + 0.009*\"day\" + 0.009*\"life\" + 0.007*\"man\" + 0.007*\"cause\" + 0.007*\"hes\" + 0.007*\"way\" + 0.007*\"fuck\" + 0.006*\"kids\" + 0.006*\"things\"\n",
      "2020-01-30 20:24:28,050 : INFO : topic #1 (0.500): 0.009*\"shit\" + 0.007*\"day\" + 0.007*\"man\" + 0.006*\"hes\" + 0.006*\"way\" + 0.006*\"lot\" + 0.006*\"thing\" + 0.006*\"life\" + 0.006*\"guy\" + 0.006*\"fuck\"\n",
      "2020-01-30 20:24:28,053 : INFO : topic diff=0.033657, rho=0.353553\n",
      "2020-01-30 20:24:28,335 : INFO : -7.602 per-word bound, 194.2 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:24:28,337 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-01-30 20:24:28,388 : INFO : topic #0 (0.500): 0.011*\"thing\" + 0.009*\"day\" + 0.009*\"life\" + 0.007*\"man\" + 0.007*\"cause\" + 0.007*\"hes\" + 0.007*\"way\" + 0.007*\"fuck\" + 0.006*\"kids\" + 0.006*\"things\"\n",
      "2020-01-30 20:24:28,390 : INFO : topic #1 (0.500): 0.009*\"shit\" + 0.007*\"day\" + 0.007*\"man\" + 0.006*\"hes\" + 0.006*\"way\" + 0.006*\"lot\" + 0.006*\"thing\" + 0.006*\"life\" + 0.006*\"guy\" + 0.006*\"fuck\"\n",
      "2020-01-30 20:24:28,393 : INFO : topic diff=0.021655, rho=0.333333\n",
      "2020-01-30 20:24:28,662 : INFO : -7.601 per-word bound, 194.1 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:24:28,664 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-01-30 20:24:28,725 : INFO : topic #0 (0.500): 0.011*\"thing\" + 0.009*\"day\" + 0.009*\"life\" + 0.007*\"cause\" + 0.007*\"man\" + 0.007*\"hes\" + 0.007*\"way\" + 0.007*\"fuck\" + 0.006*\"kids\" + 0.006*\"things\"\n",
      "2020-01-30 20:24:28,728 : INFO : topic #1 (0.500): 0.009*\"shit\" + 0.007*\"day\" + 0.007*\"man\" + 0.006*\"hes\" + 0.006*\"way\" + 0.006*\"lot\" + 0.006*\"thing\" + 0.006*\"life\" + 0.006*\"guy\" + 0.006*\"fuck\"\n",
      "2020-01-30 20:24:28,731 : INFO : topic diff=0.014260, rho=0.316228\n",
      "2020-01-30 20:24:29,039 : INFO : -7.601 per-word bound, 194.1 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:24:29,041 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-01-30 20:24:29,101 : INFO : topic #0 (0.500): 0.011*\"thing\" + 0.009*\"day\" + 0.009*\"life\" + 0.007*\"cause\" + 0.007*\"man\" + 0.007*\"hes\" + 0.007*\"way\" + 0.007*\"fuck\" + 0.006*\"kids\" + 0.006*\"things\"\n",
      "2020-01-30 20:24:29,104 : INFO : topic #1 (0.500): 0.009*\"shit\" + 0.007*\"day\" + 0.007*\"man\" + 0.006*\"hes\" + 0.006*\"way\" + 0.006*\"lot\" + 0.006*\"thing\" + 0.006*\"life\" + 0.006*\"guy\" + 0.006*\"fuck\"\n",
      "2020-01-30 20:24:29,107 : INFO : topic diff=0.009582, rho=0.301511\n",
      "2020-01-30 20:24:29,114 : INFO : topic #0 (0.500): 0.011*\"thing\" + 0.009*\"day\" + 0.009*\"life\" + 0.007*\"cause\" + 0.007*\"man\" + 0.007*\"hes\" + 0.007*\"way\" + 0.007*\"fuck\" + 0.006*\"kids\" + 0.006*\"things\"\n",
      "2020-01-30 20:24:29,118 : INFO : topic #1 (0.500): 0.009*\"shit\" + 0.007*\"day\" + 0.007*\"man\" + 0.006*\"hes\" + 0.006*\"way\" + 0.006*\"lot\" + 0.006*\"thing\" + 0.006*\"life\" + 0.006*\"guy\" + 0.006*\"fuck\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"thing\" + 0.009*\"day\" + 0.009*\"life\" + 0.007*\"cause\" + 0.007*\"man\" + 0.007*\"hes\" + 0.007*\"way\" + 0.007*\"fuck\" + 0.006*\"kids\" + 0.006*\"things\"'),\n",
       " (1,\n",
       "  '0.009*\"shit\" + 0.007*\"day\" + 0.007*\"man\" + 0.006*\"hes\" + 0.006*\"way\" + 0.006*\"lot\" + 0.006*\"thing\" + 0.006*\"life\" + 0.006*\"guy\" + 0.006*\"fuck\"')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:24:56,118 : INFO : using symmetric alpha at 0.3333333333333333\n",
      "2020-01-30 20:24:56,123 : INFO : using symmetric eta at 0.3333333333333333\n",
      "2020-01-30 20:24:56,129 : INFO : using serial LDA version on this node\n",
      "2020-01-30 20:24:56,141 : INFO : running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-01-30 20:24:57,279 : INFO : -9.164 per-word bound, 573.7 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:24:57,283 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-01-30 20:24:57,372 : INFO : topic #0 (0.333): 0.007*\"day\" + 0.007*\"way\" + 0.007*\"thing\" + 0.007*\"hes\" + 0.006*\"man\" + 0.006*\"fuck\" + 0.005*\"shit\" + 0.005*\"years\" + 0.005*\"lot\" + 0.004*\"joke\"\n",
      "2020-01-30 20:24:57,376 : INFO : topic #1 (0.333): 0.009*\"thing\" + 0.007*\"life\" + 0.007*\"day\" + 0.006*\"cause\" + 0.006*\"way\" + 0.006*\"shit\" + 0.006*\"dad\" + 0.005*\"man\" + 0.005*\"hes\" + 0.005*\"lot\"\n",
      "2020-01-30 20:24:57,380 : INFO : topic #2 (0.333): 0.010*\"shit\" + 0.010*\"day\" + 0.009*\"life\" + 0.008*\"man\" + 0.008*\"thing\" + 0.008*\"hes\" + 0.007*\"guy\" + 0.007*\"cause\" + 0.007*\"fuck\" + 0.007*\"gon\"\n",
      "2020-01-30 20:24:57,385 : INFO : topic diff=1.165001, rho=1.000000\n",
      "2020-01-30 20:24:57,873 : INFO : -7.844 per-word bound, 229.8 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:24:57,875 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-01-30 20:24:57,975 : INFO : topic #0 (0.333): 0.007*\"hes\" + 0.006*\"thing\" + 0.006*\"day\" + 0.006*\"man\" + 0.006*\"joke\" + 0.006*\"way\" + 0.005*\"stuff\" + 0.005*\"years\" + 0.004*\"things\" + 0.004*\"bo\"\n",
      "2020-01-30 20:24:57,979 : INFO : topic #1 (0.333): 0.009*\"dad\" + 0.007*\"day\" + 0.007*\"thing\" + 0.007*\"life\" + 0.006*\"school\" + 0.006*\"mom\" + 0.005*\"way\" + 0.005*\"shit\" + 0.005*\"shes\" + 0.005*\"lot\"\n",
      "2020-01-30 20:24:57,984 : INFO : topic #2 (0.333): 0.010*\"shit\" + 0.009*\"thing\" + 0.009*\"day\" + 0.009*\"man\" + 0.008*\"cause\" + 0.008*\"life\" + 0.008*\"guy\" + 0.008*\"hes\" + 0.007*\"fuck\" + 0.007*\"way\"\n",
      "2020-01-30 20:24:57,988 : INFO : topic diff=0.482444, rho=0.577350\n",
      "2020-01-30 20:24:58,407 : INFO : -7.659 per-word bound, 202.1 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:24:58,410 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-01-30 20:24:58,469 : INFO : topic #0 (0.333): 0.007*\"hes\" + 0.007*\"joke\" + 0.007*\"thing\" + 0.006*\"day\" + 0.006*\"stuff\" + 0.006*\"man\" + 0.005*\"years\" + 0.005*\"way\" + 0.005*\"bo\" + 0.005*\"things\"\n",
      "2020-01-30 20:24:58,473 : INFO : topic #1 (0.333): 0.011*\"dad\" + 0.008*\"day\" + 0.007*\"life\" + 0.007*\"school\" + 0.006*\"mom\" + 0.006*\"thing\" + 0.006*\"joke\" + 0.005*\"shes\" + 0.005*\"way\" + 0.005*\"parents\"\n",
      "2020-01-30 20:24:58,476 : INFO : topic #2 (0.333): 0.010*\"shit\" + 0.009*\"thing\" + 0.009*\"day\" + 0.009*\"cause\" + 0.009*\"man\" + 0.008*\"life\" + 0.008*\"guy\" + 0.008*\"hes\" + 0.008*\"fuck\" + 0.007*\"way\"\n",
      "2020-01-30 20:24:58,480 : INFO : topic diff=0.276690, rho=0.500000\n",
      "2020-01-30 20:24:58,880 : INFO : -7.608 per-word bound, 195.1 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:24:58,883 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-01-30 20:24:58,942 : INFO : topic #0 (0.333): 0.007*\"joke\" + 0.007*\"hes\" + 0.007*\"thing\" + 0.006*\"stuff\" + 0.006*\"day\" + 0.006*\"man\" + 0.006*\"years\" + 0.006*\"bo\" + 0.005*\"way\" + 0.005*\"repeat\"\n",
      "2020-01-30 20:24:58,946 : INFO : topic #1 (0.333): 0.012*\"dad\" + 0.008*\"day\" + 0.007*\"school\" + 0.007*\"life\" + 0.007*\"mom\" + 0.006*\"joke\" + 0.006*\"thing\" + 0.005*\"shes\" + 0.005*\"parents\" + 0.005*\"home\"\n",
      "2020-01-30 20:24:58,950 : INFO : topic #2 (0.333): 0.010*\"shit\" + 0.010*\"thing\" + 0.009*\"cause\" + 0.009*\"day\" + 0.009*\"man\" + 0.008*\"life\" + 0.008*\"guy\" + 0.008*\"fuck\" + 0.008*\"hes\" + 0.007*\"way\"\n",
      "2020-01-30 20:24:58,952 : INFO : topic diff=0.159660, rho=0.447214\n",
      "2020-01-30 20:24:59,343 : INFO : -7.591 per-word bound, 192.8 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:24:59,345 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-01-30 20:24:59,405 : INFO : topic #0 (0.333): 0.007*\"joke\" + 0.007*\"hes\" + 0.007*\"thing\" + 0.007*\"stuff\" + 0.006*\"day\" + 0.006*\"man\" + 0.006*\"bo\" + 0.006*\"years\" + 0.005*\"repeat\" + 0.005*\"id\"\n",
      "2020-01-30 20:24:59,408 : INFO : topic #1 (0.333): 0.012*\"dad\" + 0.008*\"day\" + 0.007*\"school\" + 0.007*\"mom\" + 0.007*\"life\" + 0.007*\"joke\" + 0.006*\"shes\" + 0.005*\"thing\" + 0.005*\"parents\" + 0.005*\"home\"\n",
      "2020-01-30 20:24:59,412 : INFO : topic #2 (0.333): 0.010*\"shit\" + 0.010*\"thing\" + 0.009*\"cause\" + 0.009*\"day\" + 0.009*\"man\" + 0.008*\"life\" + 0.008*\"guy\" + 0.008*\"fuck\" + 0.008*\"hes\" + 0.007*\"way\"\n",
      "2020-01-30 20:24:59,415 : INFO : topic diff=0.095692, rho=0.408248\n",
      "2020-01-30 20:24:59,713 : INFO : -7.585 per-word bound, 192.0 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:24:59,716 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-01-30 20:24:59,778 : INFO : topic #0 (0.333): 0.007*\"joke\" + 0.007*\"hes\" + 0.007*\"stuff\" + 0.007*\"thing\" + 0.006*\"day\" + 0.006*\"man\" + 0.006*\"bo\" + 0.006*\"years\" + 0.005*\"repeat\" + 0.005*\"id\"\n",
      "2020-01-30 20:24:59,782 : INFO : topic #1 (0.333): 0.012*\"dad\" + 0.008*\"day\" + 0.007*\"school\" + 0.007*\"mom\" + 0.007*\"life\" + 0.007*\"joke\" + 0.006*\"shes\" + 0.005*\"parents\" + 0.005*\"thing\" + 0.005*\"home\"\n",
      "2020-01-30 20:24:59,785 : INFO : topic #2 (0.333): 0.010*\"shit\" + 0.010*\"thing\" + 0.009*\"cause\" + 0.009*\"day\" + 0.009*\"man\" + 0.008*\"life\" + 0.008*\"guy\" + 0.008*\"fuck\" + 0.008*\"hes\" + 0.007*\"way\"\n",
      "2020-01-30 20:24:59,788 : INFO : topic diff=0.058914, rho=0.377964\n",
      "2020-01-30 20:25:00,102 : INFO : -7.582 per-word bound, 191.6 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:25:00,105 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-01-30 20:25:00,153 : INFO : topic #0 (0.333): 0.007*\"joke\" + 0.007*\"hes\" + 0.007*\"stuff\" + 0.007*\"thing\" + 0.006*\"day\" + 0.006*\"bo\" + 0.006*\"man\" + 0.006*\"years\" + 0.005*\"repeat\" + 0.005*\"id\"\n",
      "2020-01-30 20:25:00,156 : INFO : topic #1 (0.333): 0.013*\"dad\" + 0.008*\"day\" + 0.007*\"school\" + 0.007*\"mom\" + 0.007*\"joke\" + 0.007*\"life\" + 0.006*\"shes\" + 0.005*\"parents\" + 0.005*\"thing\" + 0.005*\"home\"\n",
      "2020-01-30 20:25:00,158 : INFO : topic #2 (0.333): 0.010*\"shit\" + 0.010*\"thing\" + 0.009*\"cause\" + 0.009*\"day\" + 0.009*\"man\" + 0.008*\"life\" + 0.008*\"guy\" + 0.008*\"fuck\" + 0.008*\"hes\" + 0.007*\"way\"\n",
      "2020-01-30 20:25:00,161 : INFO : topic diff=0.037148, rho=0.353553\n",
      "2020-01-30 20:25:00,473 : INFO : -7.581 per-word bound, 191.5 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:25:00,475 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-01-30 20:25:00,511 : INFO : topic #0 (0.333): 0.007*\"joke\" + 0.007*\"hes\" + 0.007*\"stuff\" + 0.007*\"thing\" + 0.006*\"day\" + 0.006*\"bo\" + 0.006*\"man\" + 0.006*\"years\" + 0.005*\"repeat\" + 0.005*\"id\"\n",
      "2020-01-30 20:25:00,514 : INFO : topic #1 (0.333): 0.013*\"dad\" + 0.008*\"day\" + 0.008*\"school\" + 0.007*\"mom\" + 0.007*\"joke\" + 0.007*\"life\" + 0.006*\"shes\" + 0.005*\"parents\" + 0.005*\"home\" + 0.005*\"family\"\n",
      "2020-01-30 20:25:00,516 : INFO : topic #2 (0.333): 0.011*\"shit\" + 0.010*\"thing\" + 0.009*\"cause\" + 0.009*\"day\" + 0.009*\"man\" + 0.008*\"life\" + 0.008*\"guy\" + 0.008*\"fuck\" + 0.008*\"hes\" + 0.007*\"way\"\n",
      "2020-01-30 20:25:00,519 : INFO : topic diff=0.023945, rho=0.333333\n",
      "2020-01-30 20:25:00,784 : INFO : -7.581 per-word bound, 191.4 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:25:00,785 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-01-30 20:25:00,827 : INFO : topic #0 (0.333): 0.007*\"joke\" + 0.007*\"hes\" + 0.007*\"stuff\" + 0.007*\"thing\" + 0.006*\"day\" + 0.006*\"bo\" + 0.006*\"man\" + 0.006*\"years\" + 0.005*\"repeat\" + 0.005*\"id\"\n",
      "2020-01-30 20:25:00,830 : INFO : topic #1 (0.333): 0.013*\"dad\" + 0.008*\"day\" + 0.008*\"school\" + 0.007*\"mom\" + 0.007*\"joke\" + 0.007*\"life\" + 0.006*\"shes\" + 0.005*\"parents\" + 0.005*\"home\" + 0.005*\"family\"\n",
      "2020-01-30 20:25:00,832 : INFO : topic #2 (0.333): 0.011*\"shit\" + 0.010*\"thing\" + 0.009*\"cause\" + 0.009*\"day\" + 0.009*\"man\" + 0.008*\"life\" + 0.008*\"guy\" + 0.008*\"fuck\" + 0.008*\"hes\" + 0.007*\"way\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:25:00,834 : INFO : topic diff=0.015753, rho=0.316228\n",
      "2020-01-30 20:25:01,117 : INFO : -7.580 per-word bound, 191.4 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:25:01,119 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-01-30 20:25:01,156 : INFO : topic #0 (0.333): 0.007*\"joke\" + 0.007*\"hes\" + 0.007*\"stuff\" + 0.007*\"thing\" + 0.006*\"day\" + 0.006*\"bo\" + 0.006*\"man\" + 0.006*\"years\" + 0.005*\"repeat\" + 0.005*\"id\"\n",
      "2020-01-30 20:25:01,158 : INFO : topic #1 (0.333): 0.013*\"dad\" + 0.008*\"day\" + 0.008*\"school\" + 0.007*\"mom\" + 0.007*\"joke\" + 0.007*\"life\" + 0.006*\"shes\" + 0.005*\"parents\" + 0.005*\"home\" + 0.005*\"family\"\n",
      "2020-01-30 20:25:01,160 : INFO : topic #2 (0.333): 0.011*\"shit\" + 0.010*\"thing\" + 0.009*\"cause\" + 0.009*\"day\" + 0.009*\"man\" + 0.008*\"life\" + 0.008*\"guy\" + 0.008*\"fuck\" + 0.008*\"hes\" + 0.007*\"way\"\n",
      "2020-01-30 20:25:01,163 : INFO : topic diff=0.010560, rho=0.301511\n",
      "2020-01-30 20:25:01,169 : INFO : topic #0 (0.333): 0.007*\"joke\" + 0.007*\"hes\" + 0.007*\"stuff\" + 0.007*\"thing\" + 0.006*\"day\" + 0.006*\"bo\" + 0.006*\"man\" + 0.006*\"years\" + 0.005*\"repeat\" + 0.005*\"id\"\n",
      "2020-01-30 20:25:01,171 : INFO : topic #1 (0.333): 0.013*\"dad\" + 0.008*\"day\" + 0.008*\"school\" + 0.007*\"mom\" + 0.007*\"joke\" + 0.007*\"life\" + 0.006*\"shes\" + 0.005*\"parents\" + 0.005*\"home\" + 0.005*\"family\"\n",
      "2020-01-30 20:25:01,173 : INFO : topic #2 (0.333): 0.011*\"shit\" + 0.010*\"thing\" + 0.009*\"cause\" + 0.009*\"day\" + 0.009*\"man\" + 0.008*\"life\" + 0.008*\"guy\" + 0.008*\"fuck\" + 0.008*\"hes\" + 0.007*\"way\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"joke\" + 0.007*\"hes\" + 0.007*\"stuff\" + 0.007*\"thing\" + 0.006*\"day\" + 0.006*\"bo\" + 0.006*\"man\" + 0.006*\"years\" + 0.005*\"repeat\" + 0.005*\"id\"'),\n",
       " (1,\n",
       "  '0.013*\"dad\" + 0.008*\"day\" + 0.008*\"school\" + 0.007*\"mom\" + 0.007*\"joke\" + 0.007*\"life\" + 0.006*\"shes\" + 0.005*\"parents\" + 0.005*\"home\" + 0.005*\"family\"'),\n",
       " (2,\n",
       "  '0.011*\"shit\" + 0.010*\"thing\" + 0.009*\"cause\" + 0.009*\"day\" + 0.009*\"man\" + 0.008*\"life\" + 0.008*\"guy\" + 0.008*\"fuck\" + 0.008*\"hes\" + 0.007*\"way\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:25:24,126 : INFO : using symmetric alpha at 0.25\n",
      "2020-01-30 20:25:24,131 : INFO : using symmetric eta at 0.25\n",
      "2020-01-30 20:25:24,137 : INFO : using serial LDA version on this node\n",
      "2020-01-30 20:25:24,151 : INFO : running online (multi-pass) LDA training, 4 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-01-30 20:25:25,513 : INFO : -9.427 per-word bound, 688.4 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:25:25,517 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-01-30 20:25:25,633 : INFO : topic #0 (0.250): 0.009*\"thing\" + 0.009*\"day\" + 0.008*\"man\" + 0.008*\"cause\" + 0.007*\"fuck\" + 0.006*\"hes\" + 0.006*\"way\" + 0.006*\"shit\" + 0.006*\"guy\" + 0.006*\"life\"\n",
      "2020-01-30 20:25:25,637 : INFO : topic #1 (0.250): 0.010*\"thing\" + 0.008*\"day\" + 0.008*\"man\" + 0.007*\"fuck\" + 0.007*\"shit\" + 0.007*\"life\" + 0.006*\"cause\" + 0.006*\"hes\" + 0.006*\"way\" + 0.005*\"lot\"\n",
      "2020-01-30 20:25:25,641 : INFO : topic #2 (0.250): 0.008*\"shit\" + 0.008*\"day\" + 0.008*\"thing\" + 0.008*\"life\" + 0.008*\"hes\" + 0.007*\"lot\" + 0.007*\"gon\" + 0.007*\"way\" + 0.007*\"man\" + 0.006*\"things\"\n",
      "2020-01-30 20:25:25,646 : INFO : topic #3 (0.250): 0.009*\"shit\" + 0.008*\"life\" + 0.007*\"hes\" + 0.007*\"guy\" + 0.007*\"day\" + 0.006*\"way\" + 0.006*\"man\" + 0.006*\"thing\" + 0.006*\"cause\" + 0.005*\"night\"\n",
      "2020-01-30 20:25:25,649 : INFO : topic diff=1.382553, rho=1.000000\n",
      "2020-01-30 20:25:26,263 : INFO : -8.051 per-word bound, 265.2 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:25:26,266 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-01-30 20:25:26,368 : INFO : topic #0 (0.250): 0.009*\"day\" + 0.008*\"dad\" + 0.008*\"thing\" + 0.007*\"way\" + 0.007*\"cause\" + 0.007*\"man\" + 0.007*\"life\" + 0.006*\"fuck\" + 0.006*\"house\" + 0.006*\"mom\"\n",
      "2020-01-30 20:25:26,371 : INFO : topic #1 (0.250): 0.011*\"thing\" + 0.010*\"day\" + 0.008*\"fuck\" + 0.008*\"shit\" + 0.007*\"man\" + 0.007*\"guy\" + 0.007*\"cause\" + 0.006*\"life\" + 0.006*\"hes\" + 0.005*\"theyre\"\n",
      "2020-01-30 20:25:26,376 : INFO : topic #2 (0.250): 0.010*\"shit\" + 0.009*\"thing\" + 0.008*\"hes\" + 0.008*\"day\" + 0.008*\"man\" + 0.008*\"lot\" + 0.008*\"life\" + 0.007*\"fuck\" + 0.006*\"gon\" + 0.006*\"way\"\n",
      "2020-01-30 20:25:26,385 : INFO : topic #3 (0.250): 0.008*\"life\" + 0.007*\"hes\" + 0.007*\"shit\" + 0.007*\"guy\" + 0.007*\"way\" + 0.006*\"cause\" + 0.006*\"man\" + 0.006*\"thing\" + 0.006*\"stuff\" + 0.006*\"day\"\n",
      "2020-01-30 20:25:26,389 : INFO : topic diff=0.627931, rho=0.577350\n",
      "2020-01-30 20:25:26,773 : INFO : -7.776 per-word bound, 219.1 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:25:26,775 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-01-30 20:25:26,832 : INFO : topic #0 (0.250): 0.009*\"dad\" + 0.009*\"day\" + 0.007*\"way\" + 0.007*\"thing\" + 0.007*\"house\" + 0.007*\"life\" + 0.007*\"man\" + 0.007*\"mom\" + 0.007*\"fuck\" + 0.006*\"cause\"\n",
      "2020-01-30 20:25:26,835 : INFO : topic #1 (0.250): 0.011*\"thing\" + 0.011*\"day\" + 0.009*\"shit\" + 0.008*\"guy\" + 0.008*\"fuck\" + 0.008*\"man\" + 0.007*\"cause\" + 0.007*\"hes\" + 0.006*\"life\" + 0.006*\"gon\"\n",
      "2020-01-30 20:25:26,838 : INFO : topic #2 (0.250): 0.010*\"shit\" + 0.009*\"thing\" + 0.008*\"hes\" + 0.008*\"man\" + 0.008*\"lot\" + 0.008*\"day\" + 0.007*\"life\" + 0.006*\"fuck\" + 0.006*\"women\" + 0.006*\"years\"\n",
      "2020-01-30 20:25:26,842 : INFO : topic #3 (0.250): 0.008*\"life\" + 0.007*\"way\" + 0.007*\"cause\" + 0.007*\"hes\" + 0.006*\"stuff\" + 0.006*\"guy\" + 0.006*\"thing\" + 0.006*\"kind\" + 0.005*\"man\" + 0.005*\"day\"\n",
      "2020-01-30 20:25:26,845 : INFO : topic diff=0.477888, rho=0.500000\n",
      "2020-01-30 20:25:27,207 : INFO : -7.650 per-word bound, 200.8 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:25:27,210 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-01-30 20:25:27,258 : INFO : topic #0 (0.250): 0.010*\"dad\" + 0.009*\"day\" + 0.008*\"house\" + 0.007*\"mom\" + 0.007*\"life\" + 0.007*\"way\" + 0.007*\"man\" + 0.007*\"fuck\" + 0.007*\"thing\" + 0.006*\"shes\"\n",
      "2020-01-30 20:25:27,261 : INFO : topic #1 (0.250): 0.012*\"thing\" + 0.011*\"day\" + 0.009*\"shit\" + 0.009*\"guy\" + 0.008*\"fuck\" + 0.008*\"man\" + 0.007*\"cause\" + 0.007*\"hes\" + 0.007*\"gon\" + 0.006*\"life\"\n",
      "2020-01-30 20:25:27,264 : INFO : topic #2 (0.250): 0.011*\"shit\" + 0.008*\"hes\" + 0.008*\"thing\" + 0.008*\"man\" + 0.008*\"lot\" + 0.008*\"day\" + 0.006*\"women\" + 0.006*\"years\" + 0.006*\"life\" + 0.006*\"fuck\"\n",
      "2020-01-30 20:25:27,267 : INFO : topic #3 (0.250): 0.009*\"life\" + 0.008*\"cause\" + 0.007*\"way\" + 0.007*\"thing\" + 0.007*\"hes\" + 0.007*\"stuff\" + 0.006*\"guy\" + 0.006*\"kind\" + 0.006*\"point\" + 0.005*\"man\"\n",
      "2020-01-30 20:25:27,271 : INFO : topic diff=0.290139, rho=0.447214\n",
      "2020-01-30 20:25:27,614 : INFO : -7.606 per-word bound, 194.8 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:25:27,616 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-01-30 20:25:27,664 : INFO : topic #0 (0.250): 0.010*\"dad\" + 0.009*\"day\" + 0.008*\"house\" + 0.008*\"mom\" + 0.008*\"life\" + 0.007*\"way\" + 0.007*\"fuck\" + 0.007*\"man\" + 0.007*\"thing\" + 0.007*\"shes\"\n",
      "2020-01-30 20:25:27,667 : INFO : topic #1 (0.250): 0.012*\"thing\" + 0.011*\"day\" + 0.010*\"shit\" + 0.009*\"guy\" + 0.008*\"fuck\" + 0.008*\"man\" + 0.007*\"cause\" + 0.007*\"hes\" + 0.007*\"gon\" + 0.006*\"life\"\n",
      "2020-01-30 20:25:27,671 : INFO : topic #2 (0.250): 0.011*\"shit\" + 0.008*\"man\" + 0.008*\"lot\" + 0.008*\"hes\" + 0.008*\"thing\" + 0.007*\"day\" + 0.006*\"years\" + 0.006*\"women\" + 0.006*\"fuck\" + 0.005*\"woman\"\n",
      "2020-01-30 20:25:27,675 : INFO : topic #3 (0.250): 0.009*\"life\" + 0.008*\"cause\" + 0.008*\"thing\" + 0.007*\"way\" + 0.007*\"hes\" + 0.007*\"stuff\" + 0.006*\"guy\" + 0.006*\"kind\" + 0.006*\"point\" + 0.005*\"gon\"\n",
      "2020-01-30 20:25:27,678 : INFO : topic diff=0.184986, rho=0.408248\n",
      "2020-01-30 20:25:28,023 : INFO : -7.586 per-word bound, 192.2 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:25:28,025 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-01-30 20:25:28,077 : INFO : topic #0 (0.250): 0.010*\"dad\" + 0.009*\"day\" + 0.008*\"house\" + 0.008*\"mom\" + 0.008*\"life\" + 0.007*\"way\" + 0.007*\"fuck\" + 0.007*\"man\" + 0.007*\"thing\" + 0.007*\"shes\"\n",
      "2020-01-30 20:25:28,080 : INFO : topic #1 (0.250): 0.012*\"thing\" + 0.011*\"day\" + 0.010*\"shit\" + 0.009*\"guy\" + 0.008*\"fuck\" + 0.008*\"man\" + 0.007*\"cause\" + 0.007*\"hes\" + 0.007*\"gon\" + 0.006*\"fucking\"\n",
      "2020-01-30 20:25:28,084 : INFO : topic #2 (0.250): 0.011*\"shit\" + 0.008*\"man\" + 0.008*\"lot\" + 0.008*\"hes\" + 0.007*\"day\" + 0.007*\"thing\" + 0.006*\"years\" + 0.006*\"women\" + 0.006*\"fuck\" + 0.006*\"woman\"\n",
      "2020-01-30 20:25:28,087 : INFO : topic #3 (0.250): 0.010*\"life\" + 0.008*\"cause\" + 0.008*\"thing\" + 0.008*\"way\" + 0.007*\"hes\" + 0.007*\"stuff\" + 0.006*\"guy\" + 0.006*\"kind\" + 0.006*\"point\" + 0.005*\"gon\"\n",
      "2020-01-30 20:25:28,091 : INFO : topic diff=0.119645, rho=0.377964\n",
      "2020-01-30 20:25:28,441 : INFO : -7.577 per-word bound, 191.0 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:25:28,443 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-01-30 20:25:28,495 : INFO : topic #0 (0.250): 0.011*\"dad\" + 0.009*\"day\" + 0.008*\"house\" + 0.008*\"mom\" + 0.008*\"life\" + 0.007*\"way\" + 0.007*\"fuck\" + 0.007*\"man\" + 0.007*\"shes\" + 0.007*\"thing\"\n",
      "2020-01-30 20:25:28,497 : INFO : topic #1 (0.250): 0.012*\"thing\" + 0.011*\"day\" + 0.010*\"shit\" + 0.009*\"guy\" + 0.008*\"fuck\" + 0.008*\"man\" + 0.007*\"cause\" + 0.007*\"hes\" + 0.007*\"gon\" + 0.006*\"fucking\"\n",
      "2020-01-30 20:25:28,501 : INFO : topic #2 (0.250): 0.011*\"shit\" + 0.008*\"man\" + 0.008*\"lot\" + 0.008*\"hes\" + 0.007*\"day\" + 0.007*\"thing\" + 0.006*\"years\" + 0.006*\"women\" + 0.006*\"fuck\" + 0.006*\"woman\"\n",
      "2020-01-30 20:25:28,504 : INFO : topic #3 (0.250): 0.010*\"life\" + 0.009*\"cause\" + 0.008*\"thing\" + 0.008*\"way\" + 0.007*\"hes\" + 0.007*\"stuff\" + 0.006*\"guy\" + 0.006*\"kind\" + 0.006*\"point\" + 0.005*\"gon\"\n",
      "2020-01-30 20:25:28,507 : INFO : topic diff=0.077691, rho=0.353553\n",
      "2020-01-30 20:25:28,923 : INFO : -7.573 per-word bound, 190.5 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:25:28,926 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-01-30 20:25:28,974 : INFO : topic #0 (0.250): 0.011*\"dad\" + 0.009*\"day\" + 0.008*\"house\" + 0.008*\"mom\" + 0.008*\"life\" + 0.007*\"way\" + 0.007*\"fuck\" + 0.007*\"man\" + 0.007*\"shes\" + 0.007*\"kids\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:25:28,977 : INFO : topic #1 (0.250): 0.012*\"thing\" + 0.012*\"day\" + 0.010*\"shit\" + 0.010*\"guy\" + 0.008*\"fuck\" + 0.008*\"man\" + 0.007*\"cause\" + 0.007*\"hes\" + 0.007*\"gon\" + 0.006*\"fucking\"\n",
      "2020-01-30 20:25:28,982 : INFO : topic #2 (0.250): 0.011*\"shit\" + 0.008*\"man\" + 0.008*\"lot\" + 0.008*\"hes\" + 0.007*\"day\" + 0.007*\"thing\" + 0.006*\"years\" + 0.006*\"women\" + 0.006*\"fuck\" + 0.006*\"woman\"\n",
      "2020-01-30 20:25:28,986 : INFO : topic #3 (0.250): 0.010*\"life\" + 0.009*\"cause\" + 0.008*\"thing\" + 0.008*\"way\" + 0.008*\"hes\" + 0.007*\"stuff\" + 0.006*\"guy\" + 0.006*\"kind\" + 0.006*\"point\" + 0.005*\"gon\"\n",
      "2020-01-30 20:25:28,990 : INFO : topic diff=0.051355, rho=0.333333\n",
      "2020-01-30 20:25:29,322 : INFO : -7.572 per-word bound, 190.2 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:25:29,324 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-01-30 20:25:29,365 : INFO : topic #0 (0.250): 0.011*\"dad\" + 0.009*\"day\" + 0.008*\"house\" + 0.008*\"mom\" + 0.008*\"life\" + 0.007*\"way\" + 0.007*\"fuck\" + 0.007*\"man\" + 0.007*\"shes\" + 0.007*\"kids\"\n",
      "2020-01-30 20:25:29,367 : INFO : topic #1 (0.250): 0.012*\"thing\" + 0.012*\"day\" + 0.010*\"shit\" + 0.010*\"guy\" + 0.008*\"fuck\" + 0.008*\"man\" + 0.007*\"cause\" + 0.007*\"hes\" + 0.007*\"gon\" + 0.006*\"fucking\"\n",
      "2020-01-30 20:25:29,370 : INFO : topic #2 (0.250): 0.011*\"shit\" + 0.008*\"man\" + 0.008*\"lot\" + 0.008*\"hes\" + 0.007*\"day\" + 0.007*\"thing\" + 0.006*\"years\" + 0.006*\"women\" + 0.006*\"fuck\" + 0.006*\"woman\"\n",
      "2020-01-30 20:25:29,373 : INFO : topic #3 (0.250): 0.010*\"life\" + 0.009*\"cause\" + 0.009*\"thing\" + 0.008*\"way\" + 0.008*\"hes\" + 0.007*\"stuff\" + 0.006*\"guy\" + 0.006*\"kind\" + 0.006*\"point\" + 0.006*\"gon\"\n",
      "2020-01-30 20:25:29,376 : INFO : topic diff=0.034485, rho=0.316228\n",
      "2020-01-30 20:25:29,663 : INFO : -7.571 per-word bound, 190.1 perplexity estimate based on a held-out corpus of 12 documents with 19855 words\n",
      "2020-01-30 20:25:29,664 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-01-30 20:25:29,698 : INFO : topic #0 (0.250): 0.011*\"dad\" + 0.009*\"day\" + 0.008*\"house\" + 0.008*\"mom\" + 0.008*\"life\" + 0.007*\"way\" + 0.007*\"fuck\" + 0.007*\"man\" + 0.007*\"shes\" + 0.007*\"kids\"\n",
      "2020-01-30 20:25:29,701 : INFO : topic #1 (0.250): 0.012*\"thing\" + 0.012*\"day\" + 0.010*\"shit\" + 0.010*\"guy\" + 0.008*\"fuck\" + 0.008*\"man\" + 0.007*\"cause\" + 0.007*\"hes\" + 0.007*\"gon\" + 0.006*\"fucking\"\n",
      "2020-01-30 20:25:29,703 : INFO : topic #2 (0.250): 0.011*\"shit\" + 0.008*\"man\" + 0.008*\"lot\" + 0.008*\"hes\" + 0.007*\"day\" + 0.007*\"thing\" + 0.006*\"years\" + 0.006*\"women\" + 0.006*\"fuck\" + 0.006*\"woman\"\n",
      "2020-01-30 20:25:29,706 : INFO : topic #3 (0.250): 0.010*\"life\" + 0.009*\"cause\" + 0.009*\"thing\" + 0.008*\"way\" + 0.008*\"hes\" + 0.007*\"stuff\" + 0.006*\"guy\" + 0.006*\"kind\" + 0.006*\"point\" + 0.006*\"gon\"\n",
      "2020-01-30 20:25:29,708 : INFO : topic diff=0.023499, rho=0.301511\n",
      "2020-01-30 20:25:29,714 : INFO : topic #0 (0.250): 0.011*\"dad\" + 0.009*\"day\" + 0.008*\"house\" + 0.008*\"mom\" + 0.008*\"life\" + 0.007*\"way\" + 0.007*\"fuck\" + 0.007*\"man\" + 0.007*\"shes\" + 0.007*\"kids\"\n",
      "2020-01-30 20:25:29,716 : INFO : topic #1 (0.250): 0.012*\"thing\" + 0.012*\"day\" + 0.010*\"shit\" + 0.010*\"guy\" + 0.008*\"fuck\" + 0.008*\"man\" + 0.007*\"cause\" + 0.007*\"hes\" + 0.007*\"gon\" + 0.006*\"fucking\"\n",
      "2020-01-30 20:25:29,720 : INFO : topic #2 (0.250): 0.011*\"shit\" + 0.008*\"man\" + 0.008*\"lot\" + 0.008*\"hes\" + 0.007*\"day\" + 0.007*\"thing\" + 0.006*\"years\" + 0.006*\"women\" + 0.006*\"fuck\" + 0.006*\"woman\"\n",
      "2020-01-30 20:25:29,723 : INFO : topic #3 (0.250): 0.010*\"life\" + 0.009*\"cause\" + 0.009*\"thing\" + 0.008*\"way\" + 0.008*\"hes\" + 0.007*\"stuff\" + 0.006*\"guy\" + 0.006*\"kind\" + 0.006*\"point\" + 0.006*\"gon\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"dad\" + 0.009*\"day\" + 0.008*\"house\" + 0.008*\"mom\" + 0.008*\"life\" + 0.007*\"way\" + 0.007*\"fuck\" + 0.007*\"man\" + 0.007*\"shes\" + 0.007*\"kids\"'),\n",
       " (1,\n",
       "  '0.012*\"thing\" + 0.012*\"day\" + 0.010*\"shit\" + 0.010*\"guy\" + 0.008*\"fuck\" + 0.008*\"man\" + 0.007*\"cause\" + 0.007*\"hes\" + 0.007*\"gon\" + 0.006*\"fucking\"'),\n",
       " (2,\n",
       "  '0.011*\"shit\" + 0.008*\"man\" + 0.008*\"lot\" + 0.008*\"hes\" + 0.007*\"day\" + 0.007*\"thing\" + 0.006*\"years\" + 0.006*\"women\" + 0.006*\"fuck\" + 0.006*\"woman\"'),\n",
       " (3,\n",
       "  '0.010*\"life\" + 0.009*\"cause\" + 0.009*\"thing\" + 0.008*\"way\" + 0.008*\"hes\" + 0.007*\"stuff\" + 0.006*\"guy\" + 0.006*\"kind\" + 0.006*\"point\" + 0.006*\"gon\"')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling - Attempt #3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ali</td>\n",
       "      <td>ladies gentlemen welcome stage ali wong hi wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>anthony</td>\n",
       "      <td>thank san francisco thank good people surprise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bill</td>\n",
       "      <td>right thank thank pleasure greater atlanta geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bo</td>\n",
       "      <td>old macdonald farm e i i o farm pig e i i snor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>dave</td>\n",
       "      <td>dirty jokes living stare most hard work profou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hasan</td>\n",
       "      <td>whats davis whats im home i netflix special la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>jim</td>\n",
       "      <td>ladies gentlemen welcome stage mr jim jefferie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>joe</td>\n",
       "      <td>ladies gentlemen joe fuck san francisco thanks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>john</td>\n",
       "      <td>right petunia august thats good right hello he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>louis</td>\n",
       "      <td>music lets lights lights thank much i i i nice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mike</td>\n",
       "      <td>wow hey thanks hey seattle nice look crazy ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ricky</td>\n",
       "      <td>hello great thank fuck thank lovely welcome im...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                transcript\n",
       "ali      ladies gentlemen welcome stage ali wong hi wel...\n",
       "anthony  thank san francisco thank good people surprise...\n",
       "bill     right thank thank pleasure greater atlanta geo...\n",
       "bo       old macdonald farm e i i o farm pig e i i snor...\n",
       "dave     dirty jokes living stare most hard work profou...\n",
       "hasan    whats davis whats im home i netflix special la...\n",
       "jim      ladies gentlemen welcome stage mr jim jefferie...\n",
       "joe      ladies gentlemen joe fuck san francisco thanks...\n",
       "john     right petunia august thats good right hello he...\n",
       "louis    music lets lights lights thank much i i i nice...\n",
       "mike     wow hey thanks hey seattle nice look crazy ins...\n",
       "ricky    hello great thank fuck thank lovely welcome im..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaaaah</th>\n",
       "      <th>aaaaahhhhhhh</th>\n",
       "      <th>aaaaauuugghhhhhh</th>\n",
       "      <th>aaaahhhhh</th>\n",
       "      <th>aah</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcs</th>\n",
       "      <th>ability</th>\n",
       "      <th>abject</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>ze</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zee</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zoo</th>\n",
       "      <th>Ã©clair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>ali</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>anthony</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bill</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>dave</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hasan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>jim</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>joe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>john</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>louis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mike</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ricky</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã 5584 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaaaah  aaaaahhhhhhh  aaaaauuugghhhhhh  aaaahhhhh  aah  abc  abcs  \\\n",
       "ali           0             0                 0          0    0    1     0   \n",
       "anthony       0             0                 0          0    0    0     0   \n",
       "bill          1             0                 0          0    0    0     1   \n",
       "bo            0             1                 1          1    0    0     0   \n",
       "dave          0             0                 0          0    0    0     0   \n",
       "hasan         0             0                 0          0    0    0     0   \n",
       "jim           0             0                 0          0    0    0     0   \n",
       "joe           0             0                 0          0    0    0     0   \n",
       "john          0             0                 0          0    0    0     0   \n",
       "louis         0             0                 0          0    3    0     0   \n",
       "mike          0             0                 0          0    0    0     0   \n",
       "ricky         0             0                 0          0    0    0     0   \n",
       "\n",
       "         ability  abject  able  ...  ze  zealand  zee  zeppelin  zero  \\\n",
       "ali            0       0     2  ...   0        0    0         0     0   \n",
       "anthony        0       0     0  ...   0       10    0         0     0   \n",
       "bill           0       0     1  ...   1        0    0         0     0   \n",
       "bo             1       0     0  ...   0        0    0         0     1   \n",
       "dave           0       0     0  ...   0        0    0         0     0   \n",
       "hasan          0       0     1  ...   0        0    2         0     0   \n",
       "jim            0       0     1  ...   0        0    0         0     0   \n",
       "joe            0       0     2  ...   0        0    0         0     0   \n",
       "john           0       0     3  ...   0        0    0         0     0   \n",
       "louis          0       0     1  ...   0        0    0         0     0   \n",
       "mike           0       0     0  ...   0        0    0         2     0   \n",
       "ricky          1       1     2  ...   0        0    0         0     0   \n",
       "\n",
       "         zillion  zombie  zombies  zoo  Ã©clair  \n",
       "ali            0       1        0    0       0  \n",
       "anthony        0       0        0    0       0  \n",
       "bill           1       1        1    0       0  \n",
       "bo             0       0        0    0       0  \n",
       "dave           0       0        0    0       0  \n",
       "hasan          0       0        0    0       0  \n",
       "jim            0       0        0    0       0  \n",
       "joe            0       0        0    0       0  \n",
       "john           0       0        0    0       1  \n",
       "louis          0       0        0    0       0  \n",
       "mike           0       0        0    0       0  \n",
       "ricky          0       0        0    1       0  \n",
       "\n",
       "[12 rows x 5584 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:28:46,762 : INFO : using symmetric alpha at 0.5\n",
      "2020-01-30 20:28:46,767 : INFO : using symmetric eta at 0.5\n",
      "2020-01-30 20:28:46,774 : INFO : using serial LDA version on this node\n",
      "2020-01-30 20:28:46,784 : INFO : running online (multi-pass) LDA training, 2 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-01-30 20:28:47,704 : INFO : -9.178 per-word bound, 579.3 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:28:47,707 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-01-30 20:28:47,862 : INFO : topic #0 (0.500): 0.003*\"joke\" + 0.003*\"ass\" + 0.003*\"mom\" + 0.003*\"parents\" + 0.002*\"dog\" + 0.002*\"dead\" + 0.002*\"son\" + 0.002*\"ok\" + 0.002*\"gun\" + 0.002*\"love\"\n",
      "2020-01-30 20:28:47,867 : INFO : topic #1 (0.500): 0.004*\"mom\" + 0.003*\"joke\" + 0.003*\"parents\" + 0.003*\"door\" + 0.002*\"friend\" + 0.002*\"jokes\" + 0.002*\"comedy\" + 0.002*\"class\" + 0.002*\"dick\" + 0.002*\"hasan\"\n",
      "2020-01-30 20:28:47,871 : INFO : topic diff=0.791106, rho=1.000000\n",
      "2020-01-30 20:28:48,595 : INFO : -8.472 per-word bound, 355.1 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:28:48,598 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-01-30 20:28:48,683 : INFO : topic #0 (0.500): 0.004*\"joke\" + 0.003*\"ass\" + 0.003*\"gun\" + 0.003*\"mom\" + 0.003*\"ok\" + 0.002*\"dog\" + 0.002*\"guns\" + 0.002*\"dead\" + 0.002*\"son\" + 0.002*\"dick\"\n",
      "2020-01-30 20:28:48,687 : INFO : topic #1 (0.500): 0.004*\"mom\" + 0.003*\"parents\" + 0.003*\"joke\" + 0.003*\"friend\" + 0.003*\"door\" + 0.002*\"comedy\" + 0.002*\"bo\" + 0.002*\"hasan\" + 0.002*\"love\" + 0.002*\"clinton\"\n",
      "2020-01-30 20:28:48,690 : INFO : topic diff=0.334207, rho=0.577350\n",
      "2020-01-30 20:28:49,271 : INFO : -8.341 per-word bound, 324.3 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:28:49,273 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-01-30 20:28:49,335 : INFO : topic #0 (0.500): 0.004*\"joke\" + 0.004*\"ass\" + 0.003*\"gun\" + 0.003*\"ok\" + 0.003*\"mom\" + 0.003*\"guns\" + 0.002*\"dog\" + 0.002*\"dick\" + 0.002*\"son\" + 0.002*\"dead\"\n",
      "2020-01-30 20:28:49,339 : INFO : topic #1 (0.500): 0.004*\"mom\" + 0.004*\"parents\" + 0.003*\"joke\" + 0.003*\"friend\" + 0.003*\"comedy\" + 0.003*\"door\" + 0.002*\"bo\" + 0.002*\"love\" + 0.002*\"hasan\" + 0.002*\"clinton\"\n",
      "2020-01-30 20:28:49,342 : INFO : topic diff=0.201996, rho=0.500000\n",
      "2020-01-30 20:28:49,770 : INFO : -8.296 per-word bound, 314.3 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:28:49,772 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-01-30 20:28:49,819 : INFO : topic #0 (0.500): 0.004*\"joke\" + 0.004*\"ass\" + 0.003*\"gun\" + 0.003*\"ok\" + 0.003*\"mom\" + 0.003*\"guns\" + 0.003*\"dick\" + 0.002*\"dog\" + 0.002*\"son\" + 0.002*\"anthony\"\n",
      "2020-01-30 20:28:49,821 : INFO : topic #1 (0.500): 0.004*\"mom\" + 0.004*\"parents\" + 0.003*\"joke\" + 0.003*\"friend\" + 0.003*\"comedy\" + 0.003*\"bo\" + 0.002*\"door\" + 0.002*\"love\" + 0.002*\"hasan\" + 0.002*\"clinton\"\n",
      "2020-01-30 20:28:49,824 : INFO : topic diff=0.109273, rho=0.447214\n",
      "2020-01-30 20:28:50,179 : INFO : -8.283 per-word bound, 311.4 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:28:50,181 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-01-30 20:28:50,237 : INFO : topic #0 (0.500): 0.004*\"joke\" + 0.004*\"ass\" + 0.003*\"gun\" + 0.003*\"ok\" + 0.003*\"guns\" + 0.003*\"mom\" + 0.003*\"dick\" + 0.002*\"dog\" + 0.002*\"son\" + 0.002*\"anthony\"\n",
      "2020-01-30 20:28:50,240 : INFO : topic #1 (0.500): 0.004*\"parents\" + 0.004*\"mom\" + 0.003*\"joke\" + 0.003*\"friend\" + 0.003*\"comedy\" + 0.003*\"bo\" + 0.002*\"door\" + 0.002*\"love\" + 0.002*\"hasan\" + 0.002*\"clinton\"\n",
      "2020-01-30 20:28:50,243 : INFO : topic diff=0.062866, rho=0.408248\n",
      "2020-01-30 20:28:50,605 : INFO : -8.278 per-word bound, 310.4 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:28:50,608 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-01-30 20:28:50,661 : INFO : topic #0 (0.500): 0.004*\"joke\" + 0.004*\"ass\" + 0.003*\"gun\" + 0.003*\"ok\" + 0.003*\"guns\" + 0.003*\"mom\" + 0.003*\"dick\" + 0.002*\"dog\" + 0.002*\"anthony\" + 0.002*\"son\"\n",
      "2020-01-30 20:28:50,664 : INFO : topic #1 (0.500): 0.004*\"parents\" + 0.004*\"mom\" + 0.003*\"joke\" + 0.003*\"friend\" + 0.003*\"comedy\" + 0.003*\"bo\" + 0.003*\"love\" + 0.002*\"door\" + 0.002*\"hasan\" + 0.002*\"clinton\"\n",
      "2020-01-30 20:28:50,667 : INFO : topic diff=0.037641, rho=0.377964\n",
      "2020-01-30 20:28:51,009 : INFO : -8.276 per-word bound, 309.9 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:28:51,011 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-01-30 20:28:51,064 : INFO : topic #0 (0.500): 0.004*\"joke\" + 0.004*\"ass\" + 0.003*\"gun\" + 0.003*\"ok\" + 0.003*\"guns\" + 0.003*\"mom\" + 0.003*\"dick\" + 0.002*\"dog\" + 0.002*\"anthony\" + 0.002*\"son\"\n",
      "2020-01-30 20:28:51,068 : INFO : topic #1 (0.500): 0.004*\"parents\" + 0.004*\"mom\" + 0.003*\"joke\" + 0.003*\"friend\" + 0.003*\"comedy\" + 0.003*\"bo\" + 0.003*\"love\" + 0.002*\"door\" + 0.002*\"hasan\" + 0.002*\"clinton\"\n",
      "2020-01-30 20:28:51,071 : INFO : topic diff=0.023271, rho=0.353553\n",
      "2020-01-30 20:28:51,427 : INFO : -8.275 per-word bound, 309.8 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:28:51,429 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-01-30 20:28:51,480 : INFO : topic #0 (0.500): 0.004*\"joke\" + 0.004*\"ass\" + 0.004*\"gun\" + 0.003*\"ok\" + 0.003*\"guns\" + 0.003*\"mom\" + 0.003*\"dick\" + 0.002*\"dog\" + 0.002*\"anthony\" + 0.002*\"son\"\n",
      "2020-01-30 20:28:51,483 : INFO : topic #1 (0.500): 0.004*\"parents\" + 0.004*\"mom\" + 0.003*\"joke\" + 0.003*\"friend\" + 0.003*\"comedy\" + 0.003*\"bo\" + 0.003*\"love\" + 0.002*\"door\" + 0.002*\"hasan\" + 0.002*\"clinton\"\n",
      "2020-01-30 20:28:51,486 : INFO : topic diff=0.014790, rho=0.333333\n",
      "2020-01-30 20:28:51,896 : INFO : -8.275 per-word bound, 309.7 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:28:51,898 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-01-30 20:28:51,947 : INFO : topic #0 (0.500): 0.004*\"joke\" + 0.004*\"ass\" + 0.004*\"gun\" + 0.003*\"ok\" + 0.003*\"guns\" + 0.003*\"mom\" + 0.003*\"dick\" + 0.002*\"dog\" + 0.002*\"anthony\" + 0.002*\"son\"\n",
      "2020-01-30 20:28:51,950 : INFO : topic #1 (0.500): 0.004*\"parents\" + 0.004*\"mom\" + 0.003*\"joke\" + 0.003*\"friend\" + 0.003*\"comedy\" + 0.003*\"bo\" + 0.003*\"love\" + 0.002*\"door\" + 0.002*\"hasan\" + 0.002*\"clinton\"\n",
      "2020-01-30 20:28:51,953 : INFO : topic diff=0.009631, rho=0.316228\n",
      "2020-01-30 20:28:52,357 : INFO : -8.275 per-word bound, 309.7 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:28:52,360 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-01-30 20:28:52,423 : INFO : topic #0 (0.500): 0.004*\"joke\" + 0.004*\"ass\" + 0.004*\"gun\" + 0.003*\"ok\" + 0.003*\"guns\" + 0.003*\"mom\" + 0.003*\"dick\" + 0.002*\"dog\" + 0.002*\"anthony\" + 0.002*\"son\"\n",
      "2020-01-30 20:28:52,424 : INFO : topic #1 (0.500): 0.004*\"parents\" + 0.004*\"mom\" + 0.003*\"joke\" + 0.003*\"friend\" + 0.003*\"comedy\" + 0.003*\"bo\" + 0.003*\"love\" + 0.002*\"door\" + 0.002*\"hasan\" + 0.002*\"clinton\"\n",
      "2020-01-30 20:28:52,424 : INFO : topic diff=0.006409, rho=0.301511\n",
      "2020-01-30 20:28:52,424 : INFO : topic #0 (0.500): 0.004*\"joke\" + 0.004*\"ass\" + 0.004*\"gun\" + 0.003*\"ok\" + 0.003*\"guns\" + 0.003*\"mom\" + 0.003*\"dick\" + 0.002*\"dog\" + 0.002*\"anthony\" + 0.002*\"son\"\n",
      "2020-01-30 20:28:52,424 : INFO : topic #1 (0.500): 0.004*\"parents\" + 0.004*\"mom\" + 0.003*\"joke\" + 0.003*\"friend\" + 0.003*\"comedy\" + 0.003*\"bo\" + 0.003*\"love\" + 0.002*\"door\" + 0.002*\"hasan\" + 0.002*\"clinton\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"joke\" + 0.004*\"ass\" + 0.004*\"gun\" + 0.003*\"ok\" + 0.003*\"guns\" + 0.003*\"mom\" + 0.003*\"dick\" + 0.002*\"dog\" + 0.002*\"anthony\" + 0.002*\"son\"'),\n",
       " (1,\n",
       "  '0.004*\"parents\" + 0.004*\"mom\" + 0.003*\"joke\" + 0.003*\"friend\" + 0.003*\"comedy\" + 0.003*\"bo\" + 0.003*\"love\" + 0.002*\"door\" + 0.002*\"hasan\" + 0.002*\"clinton\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:29:08,783 : INFO : using symmetric alpha at 0.3333333333333333\n",
      "2020-01-30 20:29:08,789 : INFO : using symmetric eta at 0.3333333333333333\n",
      "2020-01-30 20:29:08,797 : INFO : using serial LDA version on this node\n",
      "2020-01-30 20:29:08,810 : INFO : running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-01-30 20:29:09,641 : INFO : -9.461 per-word bound, 704.7 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:09,645 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-01-30 20:29:09,842 : INFO : topic #0 (0.333): 0.004*\"mom\" + 0.003*\"joke\" + 0.002*\"parents\" + 0.002*\"friend\" + 0.002*\"dog\" + 0.002*\"comedy\" + 0.002*\"door\" + 0.002*\"ok\" + 0.002*\"ass\" + 0.002*\"wife\"\n",
      "2020-01-30 20:29:09,846 : INFO : topic #1 (0.333): 0.003*\"joke\" + 0.003*\"ass\" + 0.003*\"door\" + 0.002*\"son\" + 0.002*\"mom\" + 0.002*\"parents\" + 0.002*\"friend\" + 0.002*\"class\" + 0.002*\"dog\" + 0.002*\"um\"\n",
      "2020-01-30 20:29:09,851 : INFO : topic #2 (0.333): 0.004*\"joke\" + 0.004*\"parents\" + 0.003*\"mom\" + 0.003*\"jokes\" + 0.002*\"dick\" + 0.002*\"hasan\" + 0.002*\"comedy\" + 0.002*\"love\" + 0.002*\"gun\" + 0.002*\"son\"\n",
      "2020-01-30 20:29:09,855 : INFO : topic diff=0.977452, rho=1.000000\n",
      "2020-01-30 20:29:10,485 : INFO : -8.668 per-word bound, 406.8 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:10,488 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-01-30 20:29:10,690 : INFO : topic #0 (0.333): 0.005*\"mom\" + 0.004*\"clinton\" + 0.003*\"wife\" + 0.003*\"cow\" + 0.003*\"parents\" + 0.003*\"president\" + 0.003*\"friend\" + 0.002*\"dog\" + 0.002*\"dick\" + 0.002*\"movie\"\n",
      "2020-01-30 20:29:10,693 : INFO : topic #1 (0.333): 0.003*\"ass\" + 0.003*\"um\" + 0.003*\"bo\" + 0.002*\"friend\" + 0.002*\"jenny\" + 0.002*\"door\" + 0.002*\"guns\" + 0.002*\"son\" + 0.002*\"repeat\" + 0.002*\"joke\"\n",
      "2020-01-30 20:29:10,696 : INFO : topic #2 (0.333): 0.006*\"joke\" + 0.004*\"parents\" + 0.004*\"mom\" + 0.003*\"hasan\" + 0.003*\"jokes\" + 0.002*\"gun\" + 0.002*\"comedy\" + 0.002*\"mad\" + 0.002*\"dead\" + 0.002*\"anthony\"\n",
      "2020-01-30 20:29:10,699 : INFO : topic diff=0.481717, rho=0.577350\n",
      "2020-01-30 20:29:11,015 : INFO : -8.414 per-word bound, 341.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:11,016 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-01-30 20:29:11,056 : INFO : topic #0 (0.333): 0.006*\"clinton\" + 0.006*\"mom\" + 0.004*\"cow\" + 0.004*\"wife\" + 0.004*\"parents\" + 0.003*\"president\" + 0.003*\"friend\" + 0.003*\"movie\" + 0.003*\"john\" + 0.002*\"finch\"\n",
      "2020-01-30 20:29:11,059 : INFO : topic #1 (0.333): 0.003*\"ass\" + 0.003*\"um\" + 0.003*\"bo\" + 0.003*\"friend\" + 0.003*\"jenny\" + 0.002*\"repeat\" + 0.002*\"guns\" + 0.002*\"son\" + 0.002*\"door\" + 0.002*\"eye\"\n",
      "2020-01-30 20:29:11,061 : INFO : topic #2 (0.333): 0.006*\"joke\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.003*\"hasan\" + 0.003*\"jokes\" + 0.003*\"gun\" + 0.003*\"dead\" + 0.002*\"anthony\" + 0.002*\"hell\" + 0.002*\"nuts\"\n",
      "2020-01-30 20:29:11,064 : INFO : topic diff=0.308973, rho=0.500000\n",
      "2020-01-30 20:29:11,391 : INFO : -8.320 per-word bound, 319.5 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:11,394 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-01-30 20:29:11,436 : INFO : topic #0 (0.333): 0.007*\"clinton\" + 0.006*\"mom\" + 0.005*\"cow\" + 0.005*\"wife\" + 0.004*\"parents\" + 0.004*\"president\" + 0.003*\"movie\" + 0.003*\"friend\" + 0.003*\"john\" + 0.003*\"finch\"\n",
      "2020-01-30 20:29:11,438 : INFO : topic #1 (0.333): 0.003*\"ass\" + 0.003*\"um\" + 0.003*\"bo\" + 0.003*\"friend\" + 0.003*\"jenny\" + 0.003*\"repeat\" + 0.003*\"guns\" + 0.002*\"son\" + 0.002*\"eye\" + 0.002*\"door\"\n",
      "2020-01-30 20:29:11,441 : INFO : topic #2 (0.333): 0.007*\"joke\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.003*\"hasan\" + 0.003*\"jokes\" + 0.003*\"dead\" + 0.003*\"gun\" + 0.003*\"anthony\" + 0.003*\"hell\" + 0.003*\"nuts\"\n",
      "2020-01-30 20:29:11,444 : INFO : topic diff=0.182679, rho=0.447214\n",
      "2020-01-30 20:29:11,736 : INFO : -8.288 per-word bound, 312.5 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:11,738 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-01-30 20:29:11,772 : INFO : topic #0 (0.333): 0.008*\"clinton\" + 0.006*\"mom\" + 0.005*\"cow\" + 0.005*\"wife\" + 0.004*\"parents\" + 0.004*\"president\" + 0.003*\"movie\" + 0.003*\"john\" + 0.003*\"friend\" + 0.003*\"finch\"\n",
      "2020-01-30 20:29:11,774 : INFO : topic #1 (0.333): 0.003*\"ass\" + 0.003*\"um\" + 0.003*\"bo\" + 0.003*\"friend\" + 0.003*\"jenny\" + 0.003*\"repeat\" + 0.003*\"guns\" + 0.002*\"son\" + 0.002*\"ahah\" + 0.002*\"eye\"\n",
      "2020-01-30 20:29:11,777 : INFO : topic #2 (0.333): 0.007*\"joke\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.003*\"hasan\" + 0.003*\"jokes\" + 0.003*\"dead\" + 0.003*\"gun\" + 0.003*\"anthony\" + 0.003*\"hell\" + 0.003*\"nuts\"\n",
      "2020-01-30 20:29:11,780 : INFO : topic diff=0.113056, rho=0.408248\n",
      "2020-01-30 20:29:12,087 : INFO : -8.275 per-word bound, 309.8 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:12,089 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-01-30 20:29:12,127 : INFO : topic #0 (0.333): 0.008*\"clinton\" + 0.006*\"mom\" + 0.006*\"cow\" + 0.005*\"wife\" + 0.004*\"parents\" + 0.004*\"president\" + 0.003*\"movie\" + 0.003*\"john\" + 0.003*\"friend\" + 0.003*\"finch\"\n",
      "2020-01-30 20:29:12,130 : INFO : topic #1 (0.333): 0.003*\"ass\" + 0.003*\"um\" + 0.003*\"bo\" + 0.003*\"friend\" + 0.003*\"jenny\" + 0.003*\"repeat\" + 0.003*\"guns\" + 0.002*\"ahah\" + 0.002*\"son\" + 0.002*\"eye\"\n",
      "2020-01-30 20:29:12,132 : INFO : topic #2 (0.333): 0.007*\"joke\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.003*\"hasan\" + 0.003*\"jokes\" + 0.003*\"dead\" + 0.003*\"gun\" + 0.003*\"anthony\" + 0.003*\"hell\" + 0.003*\"nuts\"\n",
      "2020-01-30 20:29:12,134 : INFO : topic diff=0.071656, rho=0.377964\n",
      "2020-01-30 20:29:12,417 : INFO : -8.270 per-word bound, 308.6 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:12,419 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-01-30 20:29:12,453 : INFO : topic #0 (0.333): 0.008*\"clinton\" + 0.006*\"mom\" + 0.006*\"cow\" + 0.005*\"wife\" + 0.004*\"parents\" + 0.004*\"president\" + 0.003*\"movie\" + 0.003*\"john\" + 0.003*\"friend\" + 0.003*\"finch\"\n",
      "2020-01-30 20:29:12,455 : INFO : topic #1 (0.333): 0.003*\"ass\" + 0.003*\"um\" + 0.003*\"bo\" + 0.003*\"friend\" + 0.003*\"jenny\" + 0.003*\"repeat\" + 0.003*\"guns\" + 0.003*\"ahah\" + 0.002*\"son\" + 0.002*\"eye\"\n",
      "2020-01-30 20:29:12,457 : INFO : topic #2 (0.333): 0.007*\"joke\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.003*\"hasan\" + 0.003*\"jokes\" + 0.003*\"dead\" + 0.003*\"gun\" + 0.003*\"anthony\" + 0.003*\"hell\" + 0.003*\"nuts\"\n",
      "2020-01-30 20:29:12,460 : INFO : topic diff=0.046276, rho=0.353553\n",
      "2020-01-30 20:29:12,761 : INFO : -8.267 per-word bound, 308.1 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:12,763 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-01-30 20:29:12,816 : INFO : topic #0 (0.333): 0.008*\"clinton\" + 0.006*\"mom\" + 0.006*\"cow\" + 0.005*\"wife\" + 0.004*\"parents\" + 0.004*\"president\" + 0.004*\"movie\" + 0.004*\"john\" + 0.003*\"friend\" + 0.003*\"finch\"\n",
      "2020-01-30 20:29:12,820 : INFO : topic #1 (0.333): 0.003*\"ass\" + 0.003*\"um\" + 0.003*\"bo\" + 0.003*\"friend\" + 0.003*\"jenny\" + 0.003*\"repeat\" + 0.003*\"guns\" + 0.003*\"ahah\" + 0.002*\"son\" + 0.002*\"eye\"\n",
      "2020-01-30 20:29:12,823 : INFO : topic #2 (0.333): 0.007*\"joke\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.003*\"hasan\" + 0.003*\"jokes\" + 0.003*\"dead\" + 0.003*\"gun\" + 0.003*\"anthony\" + 0.003*\"hell\" + 0.003*\"nuts\"\n",
      "2020-01-30 20:29:12,825 : INFO : topic diff=0.030395, rho=0.333333\n",
      "2020-01-30 20:29:13,161 : INFO : -8.266 per-word bound, 307.9 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:13,163 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-01-30 20:29:13,205 : INFO : topic #0 (0.333): 0.008*\"clinton\" + 0.007*\"mom\" + 0.006*\"cow\" + 0.005*\"wife\" + 0.004*\"parents\" + 0.004*\"president\" + 0.004*\"movie\" + 0.004*\"john\" + 0.003*\"friend\" + 0.003*\"finch\"\n",
      "2020-01-30 20:29:13,209 : INFO : topic #1 (0.333): 0.003*\"ass\" + 0.003*\"um\" + 0.003*\"bo\" + 0.003*\"friend\" + 0.003*\"jenny\" + 0.003*\"repeat\" + 0.003*\"guns\" + 0.003*\"ahah\" + 0.002*\"son\" + 0.002*\"eye\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:29:13,213 : INFO : topic #2 (0.333): 0.007*\"joke\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.003*\"hasan\" + 0.003*\"jokes\" + 0.003*\"dead\" + 0.003*\"gun\" + 0.003*\"anthony\" + 0.003*\"hell\" + 0.003*\"nuts\"\n",
      "2020-01-30 20:29:13,217 : INFO : topic diff=0.020286, rho=0.316228\n",
      "2020-01-30 20:29:13,532 : INFO : -8.266 per-word bound, 307.8 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:13,535 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-01-30 20:29:13,573 : INFO : topic #0 (0.333): 0.008*\"clinton\" + 0.007*\"mom\" + 0.006*\"cow\" + 0.005*\"wife\" + 0.004*\"parents\" + 0.004*\"president\" + 0.004*\"movie\" + 0.004*\"john\" + 0.003*\"friend\" + 0.003*\"finch\"\n",
      "2020-01-30 20:29:13,575 : INFO : topic #1 (0.333): 0.003*\"ass\" + 0.003*\"um\" + 0.003*\"bo\" + 0.003*\"friend\" + 0.003*\"jenny\" + 0.003*\"repeat\" + 0.003*\"guns\" + 0.003*\"ahah\" + 0.002*\"son\" + 0.002*\"eye\"\n",
      "2020-01-30 20:29:13,577 : INFO : topic #2 (0.333): 0.007*\"joke\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.003*\"hasan\" + 0.003*\"jokes\" + 0.003*\"dead\" + 0.003*\"gun\" + 0.003*\"anthony\" + 0.003*\"hell\" + 0.003*\"nuts\"\n",
      "2020-01-30 20:29:13,579 : INFO : topic diff=0.013749, rho=0.301511\n",
      "2020-01-30 20:29:13,586 : INFO : topic #0 (0.333): 0.008*\"clinton\" + 0.007*\"mom\" + 0.006*\"cow\" + 0.005*\"wife\" + 0.004*\"parents\" + 0.004*\"president\" + 0.004*\"movie\" + 0.004*\"john\" + 0.003*\"friend\" + 0.003*\"finch\"\n",
      "2020-01-30 20:29:13,590 : INFO : topic #1 (0.333): 0.003*\"ass\" + 0.003*\"um\" + 0.003*\"bo\" + 0.003*\"friend\" + 0.003*\"jenny\" + 0.003*\"repeat\" + 0.003*\"guns\" + 0.003*\"ahah\" + 0.002*\"son\" + 0.002*\"eye\"\n",
      "2020-01-30 20:29:13,594 : INFO : topic #2 (0.333): 0.007*\"joke\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.003*\"hasan\" + 0.003*\"jokes\" + 0.003*\"dead\" + 0.003*\"gun\" + 0.003*\"anthony\" + 0.003*\"hell\" + 0.003*\"nuts\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"clinton\" + 0.007*\"mom\" + 0.006*\"cow\" + 0.005*\"wife\" + 0.004*\"parents\" + 0.004*\"president\" + 0.004*\"movie\" + 0.004*\"john\" + 0.003*\"friend\" + 0.003*\"finch\"'),\n",
       " (1,\n",
       "  '0.003*\"ass\" + 0.003*\"um\" + 0.003*\"bo\" + 0.003*\"friend\" + 0.003*\"jenny\" + 0.003*\"repeat\" + 0.003*\"guns\" + 0.003*\"ahah\" + 0.002*\"son\" + 0.002*\"eye\"'),\n",
       " (2,\n",
       "  '0.007*\"joke\" + 0.004*\"mom\" + 0.004*\"parents\" + 0.003*\"hasan\" + 0.003*\"jokes\" + 0.003*\"dead\" + 0.003*\"gun\" + 0.003*\"anthony\" + 0.003*\"hell\" + 0.003*\"nuts\"')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:29:35,257 : INFO : using symmetric alpha at 0.25\n",
      "2020-01-30 20:29:35,261 : INFO : using symmetric eta at 0.25\n",
      "2020-01-30 20:29:35,268 : INFO : using serial LDA version on this node\n",
      "2020-01-30 20:29:35,284 : INFO : running online (multi-pass) LDA training, 4 topics, 10 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-01-30 20:29:36,236 : INFO : -9.792 per-word bound, 886.8 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:36,239 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-01-30 20:29:36,392 : INFO : topic #0 (0.250): 0.004*\"joke\" + 0.003*\"mom\" + 0.003*\"parents\" + 0.003*\"bo\" + 0.002*\"class\" + 0.002*\"comedy\" + 0.002*\"dog\" + 0.002*\"door\" + 0.002*\"love\" + 0.002*\"ass\"\n",
      "2020-01-30 20:29:36,396 : INFO : topic #1 (0.250): 0.004*\"mom\" + 0.003*\"parents\" + 0.003*\"joke\" + 0.002*\"ass\" + 0.002*\"door\" + 0.002*\"comedy\" + 0.002*\"wife\" + 0.002*\"dog\" + 0.002*\"friend\" + 0.002*\"love\"\n",
      "2020-01-30 20:29:36,400 : INFO : topic #2 (0.250): 0.004*\"joke\" + 0.003*\"parents\" + 0.003*\"son\" + 0.003*\"mom\" + 0.003*\"ass\" + 0.002*\"friend\" + 0.002*\"mad\" + 0.002*\"dead\" + 0.002*\"jokes\" + 0.002*\"gun\"\n",
      "2020-01-30 20:29:36,405 : INFO : topic #3 (0.250): 0.004*\"joke\" + 0.003*\"mom\" + 0.003*\"jokes\" + 0.002*\"parents\" + 0.002*\"friend\" + 0.002*\"jenny\" + 0.002*\"door\" + 0.002*\"dick\" + 0.002*\"ass\" + 0.002*\"dog\"\n",
      "2020-01-30 20:29:36,409 : INFO : topic diff=1.196958, rho=1.000000\n",
      "2020-01-30 20:29:37,031 : INFO : -8.890 per-word bound, 474.3 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:37,034 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-01-30 20:29:37,232 : INFO : topic #0 (0.250): 0.004*\"joke\" + 0.004*\"mom\" + 0.003*\"parents\" + 0.003*\"hasan\" + 0.003*\"dog\" + 0.003*\"bo\" + 0.002*\"class\" + 0.002*\"comedy\" + 0.002*\"youve\" + 0.002*\"love\"\n",
      "2020-01-30 20:29:37,235 : INFO : topic #1 (0.250): 0.005*\"mom\" + 0.003*\"parents\" + 0.003*\"clinton\" + 0.003*\"wife\" + 0.002*\"president\" + 0.002*\"friend\" + 0.002*\"ass\" + 0.002*\"dog\" + 0.002*\"cow\" + 0.002*\"dick\"\n",
      "2020-01-30 20:29:37,242 : INFO : topic #2 (0.250): 0.005*\"joke\" + 0.003*\"gun\" + 0.003*\"son\" + 0.003*\"mad\" + 0.003*\"anthony\" + 0.003*\"ahah\" + 0.003*\"gay\" + 0.003*\"mom\" + 0.003*\"ass\" + 0.002*\"wife\"\n",
      "2020-01-30 20:29:37,248 : INFO : topic #3 (0.250): 0.004*\"jenny\" + 0.003*\"friend\" + 0.003*\"texas\" + 0.003*\"door\" + 0.003*\"stupid\" + 0.003*\"parents\" + 0.002*\"andy\" + 0.002*\"accident\" + 0.002*\"dick\" + 0.002*\"jokes\"\n",
      "2020-01-30 20:29:37,252 : INFO : topic diff=0.649456, rho=0.577350\n",
      "2020-01-30 20:29:37,754 : INFO : -8.507 per-word bound, 363.7 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:37,756 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-01-30 20:29:37,821 : INFO : topic #0 (0.250): 0.004*\"joke\" + 0.003*\"parents\" + 0.003*\"bo\" + 0.003*\"mom\" + 0.003*\"hasan\" + 0.003*\"dog\" + 0.003*\"comedy\" + 0.003*\"class\" + 0.003*\"youve\" + 0.003*\"repeat\"\n",
      "2020-01-30 20:29:37,824 : INFO : topic #1 (0.250): 0.006*\"mom\" + 0.006*\"clinton\" + 0.004*\"wife\" + 0.004*\"cow\" + 0.004*\"parents\" + 0.003*\"president\" + 0.003*\"friend\" + 0.003*\"movie\" + 0.003*\"john\" + 0.002*\"petunia\"\n",
      "2020-01-30 20:29:37,828 : INFO : topic #2 (0.250): 0.005*\"joke\" + 0.004*\"gun\" + 0.004*\"mad\" + 0.003*\"ahah\" + 0.003*\"son\" + 0.003*\"anthony\" + 0.003*\"gay\" + 0.003*\"mom\" + 0.003*\"husband\" + 0.003*\"wife\"\n",
      "2020-01-30 20:29:37,832 : INFO : topic #3 (0.250): 0.005*\"jenny\" + 0.003*\"texas\" + 0.003*\"friend\" + 0.003*\"door\" + 0.003*\"accident\" + 0.003*\"stupid\" + 0.003*\"parents\" + 0.003*\"andy\" + 0.002*\"dick\" + 0.002*\"scrambler\"\n",
      "2020-01-30 20:29:37,835 : INFO : topic diff=0.458223, rho=0.500000\n",
      "2020-01-30 20:29:38,284 : INFO : -8.343 per-word bound, 324.6 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:38,286 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-01-30 20:29:38,339 : INFO : topic #0 (0.250): 0.004*\"joke\" + 0.003*\"bo\" + 0.003*\"hasan\" + 0.003*\"parents\" + 0.003*\"dog\" + 0.003*\"mom\" + 0.003*\"repeat\" + 0.003*\"um\" + 0.003*\"youve\" + 0.003*\"comedy\"\n",
      "2020-01-30 20:29:38,345 : INFO : topic #1 (0.250): 0.008*\"clinton\" + 0.007*\"mom\" + 0.005*\"cow\" + 0.005*\"wife\" + 0.004*\"parents\" + 0.004*\"president\" + 0.003*\"movie\" + 0.003*\"friend\" + 0.003*\"john\" + 0.003*\"petunia\"\n",
      "2020-01-30 20:29:38,349 : INFO : topic #2 (0.250): 0.005*\"joke\" + 0.004*\"mad\" + 0.004*\"ahah\" + 0.004*\"gun\" + 0.003*\"anthony\" + 0.003*\"son\" + 0.003*\"gay\" + 0.003*\"mom\" + 0.003*\"husband\" + 0.003*\"grandma\"\n",
      "2020-01-30 20:29:38,353 : INFO : topic #3 (0.250): 0.006*\"jenny\" + 0.004*\"texas\" + 0.003*\"friend\" + 0.003*\"accident\" + 0.003*\"door\" + 0.003*\"stupid\" + 0.003*\"parents\" + 0.003*\"scrambler\" + 0.003*\"dick\" + 0.002*\"high\"\n",
      "2020-01-30 20:29:38,355 : INFO : topic diff=0.280295, rho=0.447214\n",
      "2020-01-30 20:29:38,741 : INFO : -8.287 per-word bound, 312.3 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:38,744 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-01-30 20:29:38,790 : INFO : topic #0 (0.250): 0.004*\"joke\" + 0.003*\"bo\" + 0.003*\"hasan\" + 0.003*\"parents\" + 0.003*\"dog\" + 0.003*\"repeat\" + 0.003*\"mom\" + 0.003*\"um\" + 0.003*\"youve\" + 0.003*\"eye\"\n",
      "2020-01-30 20:29:38,793 : INFO : topic #1 (0.250): 0.009*\"clinton\" + 0.007*\"mom\" + 0.006*\"cow\" + 0.006*\"wife\" + 0.005*\"parents\" + 0.004*\"president\" + 0.004*\"movie\" + 0.004*\"john\" + 0.004*\"friend\" + 0.003*\"petunia\"\n",
      "2020-01-30 20:29:38,796 : INFO : topic #2 (0.250): 0.006*\"joke\" + 0.004*\"mad\" + 0.004*\"ahah\" + 0.004*\"gun\" + 0.004*\"anthony\" + 0.003*\"son\" + 0.003*\"gay\" + 0.003*\"mom\" + 0.003*\"husband\" + 0.003*\"grandma\"\n",
      "2020-01-30 20:29:38,799 : INFO : topic #3 (0.250): 0.006*\"jenny\" + 0.004*\"texas\" + 0.003*\"friend\" + 0.003*\"accident\" + 0.003*\"door\" + 0.003*\"stupid\" + 0.003*\"parents\" + 0.003*\"scrambler\" + 0.003*\"dick\" + 0.003*\"high\"\n",
      "2020-01-30 20:29:38,803 : INFO : topic diff=0.177210, rho=0.408248\n",
      "2020-01-30 20:29:39,163 : INFO : -8.264 per-word bound, 307.5 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:39,165 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-01-30 20:29:39,210 : INFO : topic #0 (0.250): 0.004*\"joke\" + 0.004*\"bo\" + 0.003*\"hasan\" + 0.003*\"parents\" + 0.003*\"dog\" + 0.003*\"repeat\" + 0.003*\"um\" + 0.003*\"mom\" + 0.003*\"guns\" + 0.003*\"youve\"\n",
      "2020-01-30 20:29:39,214 : INFO : topic #1 (0.250): 0.009*\"clinton\" + 0.007*\"mom\" + 0.006*\"cow\" + 0.006*\"wife\" + 0.005*\"parents\" + 0.004*\"president\" + 0.004*\"movie\" + 0.004*\"john\" + 0.004*\"friend\" + 0.004*\"petunia\"\n",
      "2020-01-30 20:29:39,217 : INFO : topic #2 (0.250): 0.006*\"joke\" + 0.004*\"mad\" + 0.004*\"ahah\" + 0.004*\"anthony\" + 0.004*\"gun\" + 0.003*\"gay\" + 0.003*\"son\" + 0.003*\"mom\" + 0.003*\"husband\" + 0.003*\"grandma\"\n",
      "2020-01-30 20:29:39,222 : INFO : topic #3 (0.250): 0.006*\"jenny\" + 0.004*\"texas\" + 0.003*\"friend\" + 0.003*\"accident\" + 0.003*\"door\" + 0.003*\"stupid\" + 0.003*\"parents\" + 0.003*\"scrambler\" + 0.003*\"dick\" + 0.003*\"high\"\n",
      "2020-01-30 20:29:39,225 : INFO : topic diff=0.114180, rho=0.377964\n",
      "2020-01-30 20:29:39,555 : INFO : -8.255 per-word bound, 305.4 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:39,557 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-01-30 20:29:39,601 : INFO : topic #0 (0.250): 0.004*\"joke\" + 0.004*\"bo\" + 0.003*\"hasan\" + 0.003*\"parents\" + 0.003*\"dog\" + 0.003*\"repeat\" + 0.003*\"um\" + 0.003*\"guns\" + 0.003*\"youve\" + 0.003*\"eye\"\n",
      "2020-01-30 20:29:39,604 : INFO : topic #1 (0.250): 0.009*\"clinton\" + 0.007*\"mom\" + 0.006*\"cow\" + 0.006*\"wife\" + 0.005*\"parents\" + 0.004*\"president\" + 0.004*\"movie\" + 0.004*\"john\" + 0.004*\"friend\" + 0.004*\"petunia\"\n",
      "2020-01-30 20:29:39,607 : INFO : topic #2 (0.250): 0.006*\"joke\" + 0.004*\"mad\" + 0.004*\"ahah\" + 0.004*\"anthony\" + 0.004*\"gun\" + 0.003*\"gay\" + 0.003*\"son\" + 0.003*\"mom\" + 0.003*\"husband\" + 0.003*\"grandma\"\n",
      "2020-01-30 20:29:39,611 : INFO : topic #3 (0.250): 0.006*\"jenny\" + 0.004*\"texas\" + 0.003*\"friend\" + 0.003*\"accident\" + 0.003*\"door\" + 0.003*\"stupid\" + 0.003*\"parents\" + 0.003*\"scrambler\" + 0.003*\"dick\" + 0.003*\"high\"\n",
      "2020-01-30 20:29:39,615 : INFO : topic diff=0.074667, rho=0.353553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:29:40,045 : INFO : -8.250 per-word bound, 304.5 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:40,048 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-01-30 20:29:40,091 : INFO : topic #0 (0.250): 0.004*\"joke\" + 0.004*\"bo\" + 0.003*\"hasan\" + 0.003*\"parents\" + 0.003*\"dog\" + 0.003*\"repeat\" + 0.003*\"um\" + 0.003*\"guns\" + 0.003*\"youve\" + 0.003*\"eye\"\n",
      "2020-01-30 20:29:40,094 : INFO : topic #1 (0.250): 0.010*\"clinton\" + 0.007*\"mom\" + 0.007*\"cow\" + 0.006*\"wife\" + 0.005*\"parents\" + 0.004*\"president\" + 0.004*\"movie\" + 0.004*\"john\" + 0.004*\"friend\" + 0.004*\"petunia\"\n",
      "2020-01-30 20:29:40,098 : INFO : topic #2 (0.250): 0.006*\"joke\" + 0.004*\"mad\" + 0.004*\"ahah\" + 0.004*\"anthony\" + 0.004*\"gun\" + 0.003*\"gay\" + 0.003*\"son\" + 0.003*\"mom\" + 0.003*\"husband\" + 0.003*\"grandma\"\n",
      "2020-01-30 20:29:40,101 : INFO : topic #3 (0.250): 0.006*\"jenny\" + 0.004*\"texas\" + 0.003*\"friend\" + 0.003*\"accident\" + 0.003*\"door\" + 0.003*\"stupid\" + 0.003*\"parents\" + 0.003*\"scrambler\" + 0.003*\"dick\" + 0.003*\"high\"\n",
      "2020-01-30 20:29:40,104 : INFO : topic diff=0.049518, rho=0.333333\n",
      "2020-01-30 20:29:40,520 : INFO : -8.248 per-word bound, 304.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:40,522 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-01-30 20:29:40,563 : INFO : topic #0 (0.250): 0.004*\"joke\" + 0.004*\"bo\" + 0.003*\"hasan\" + 0.003*\"parents\" + 0.003*\"dog\" + 0.003*\"repeat\" + 0.003*\"um\" + 0.003*\"guns\" + 0.003*\"youve\" + 0.003*\"eye\"\n",
      "2020-01-30 20:29:40,566 : INFO : topic #1 (0.250): 0.010*\"clinton\" + 0.007*\"mom\" + 0.007*\"cow\" + 0.006*\"wife\" + 0.005*\"parents\" + 0.004*\"president\" + 0.004*\"movie\" + 0.004*\"john\" + 0.004*\"friend\" + 0.004*\"petunia\"\n",
      "2020-01-30 20:29:40,570 : INFO : topic #2 (0.250): 0.006*\"joke\" + 0.004*\"mad\" + 0.004*\"ahah\" + 0.004*\"anthony\" + 0.004*\"gun\" + 0.003*\"gay\" + 0.003*\"son\" + 0.003*\"mom\" + 0.003*\"husband\" + 0.003*\"grandma\"\n",
      "2020-01-30 20:29:40,573 : INFO : topic #3 (0.250): 0.006*\"jenny\" + 0.004*\"texas\" + 0.003*\"friend\" + 0.003*\"accident\" + 0.003*\"door\" + 0.003*\"stupid\" + 0.003*\"parents\" + 0.003*\"scrambler\" + 0.003*\"dick\" + 0.003*\"high\"\n",
      "2020-01-30 20:29:40,576 : INFO : topic diff=0.033298, rho=0.316228\n",
      "2020-01-30 20:29:40,973 : INFO : -8.247 per-word bound, 303.8 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:29:40,975 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-01-30 20:29:41,021 : INFO : topic #0 (0.250): 0.004*\"joke\" + 0.004*\"bo\" + 0.003*\"hasan\" + 0.003*\"parents\" + 0.003*\"dog\" + 0.003*\"repeat\" + 0.003*\"um\" + 0.003*\"guns\" + 0.003*\"youve\" + 0.003*\"eye\"\n",
      "2020-01-30 20:29:41,024 : INFO : topic #1 (0.250): 0.010*\"clinton\" + 0.008*\"mom\" + 0.007*\"cow\" + 0.006*\"wife\" + 0.005*\"parents\" + 0.004*\"president\" + 0.004*\"movie\" + 0.004*\"john\" + 0.004*\"friend\" + 0.004*\"petunia\"\n",
      "2020-01-30 20:29:41,027 : INFO : topic #2 (0.250): 0.006*\"joke\" + 0.004*\"mad\" + 0.004*\"ahah\" + 0.004*\"anthony\" + 0.004*\"gun\" + 0.003*\"gay\" + 0.003*\"son\" + 0.003*\"mom\" + 0.003*\"husband\" + 0.003*\"grandma\"\n",
      "2020-01-30 20:29:41,030 : INFO : topic #3 (0.250): 0.006*\"jenny\" + 0.004*\"texas\" + 0.003*\"friend\" + 0.003*\"accident\" + 0.003*\"door\" + 0.003*\"stupid\" + 0.003*\"parents\" + 0.003*\"scrambler\" + 0.003*\"dick\" + 0.003*\"high\"\n",
      "2020-01-30 20:29:41,033 : INFO : topic diff=0.022699, rho=0.301511\n",
      "2020-01-30 20:29:41,041 : INFO : topic #0 (0.250): 0.004*\"joke\" + 0.004*\"bo\" + 0.003*\"hasan\" + 0.003*\"parents\" + 0.003*\"dog\" + 0.003*\"repeat\" + 0.003*\"um\" + 0.003*\"guns\" + 0.003*\"youve\" + 0.003*\"eye\"\n",
      "2020-01-30 20:29:41,044 : INFO : topic #1 (0.250): 0.010*\"clinton\" + 0.008*\"mom\" + 0.007*\"cow\" + 0.006*\"wife\" + 0.005*\"parents\" + 0.004*\"president\" + 0.004*\"movie\" + 0.004*\"john\" + 0.004*\"friend\" + 0.004*\"petunia\"\n",
      "2020-01-30 20:29:41,047 : INFO : topic #2 (0.250): 0.006*\"joke\" + 0.004*\"mad\" + 0.004*\"ahah\" + 0.004*\"anthony\" + 0.004*\"gun\" + 0.003*\"gay\" + 0.003*\"son\" + 0.003*\"mom\" + 0.003*\"husband\" + 0.003*\"grandma\"\n",
      "2020-01-30 20:29:41,050 : INFO : topic #3 (0.250): 0.006*\"jenny\" + 0.004*\"texas\" + 0.003*\"friend\" + 0.003*\"accident\" + 0.003*\"door\" + 0.003*\"stupid\" + 0.003*\"parents\" + 0.003*\"scrambler\" + 0.003*\"dick\" + 0.003*\"high\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"joke\" + 0.004*\"bo\" + 0.003*\"hasan\" + 0.003*\"parents\" + 0.003*\"dog\" + 0.003*\"repeat\" + 0.003*\"um\" + 0.003*\"guns\" + 0.003*\"youve\" + 0.003*\"eye\"'),\n",
       " (1,\n",
       "  '0.010*\"clinton\" + 0.008*\"mom\" + 0.007*\"cow\" + 0.006*\"wife\" + 0.005*\"parents\" + 0.004*\"president\" + 0.004*\"movie\" + 0.004*\"john\" + 0.004*\"friend\" + 0.004*\"petunia\"'),\n",
       " (2,\n",
       "  '0.006*\"joke\" + 0.004*\"mad\" + 0.004*\"ahah\" + 0.004*\"anthony\" + 0.004*\"gun\" + 0.003*\"gay\" + 0.003*\"son\" + 0.003*\"mom\" + 0.003*\"husband\" + 0.003*\"grandma\"'),\n",
       " (3,\n",
       "  '0.006*\"jenny\" + 0.004*\"texas\" + 0.003*\"friend\" + 0.003*\"accident\" + 0.003*\"door\" + 0.003*\"stupid\" + 0.003*\"parents\" + 0.003*\"scrambler\" + 0.003*\"dick\" + 0.003*\"high\"')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Topics in Each Document\n",
    "Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:30:33,842 : INFO : using symmetric alpha at 0.25\n",
      "2020-01-30 20:30:33,847 : INFO : using symmetric eta at 0.25\n",
      "2020-01-30 20:30:33,854 : INFO : using serial LDA version on this node\n",
      "2020-01-30 20:30:33,870 : INFO : running online (multi-pass) LDA training, 4 topics, 80 passes over the supplied corpus of 12 documents, updating model once every 12 documents, evaluating perplexity every 12 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2020-01-30 20:30:34,757 : INFO : -9.796 per-word bound, 888.9 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:34,757 : INFO : PROGRESS: pass 0, at document #12/12\n",
      "2020-01-30 20:30:34,938 : INFO : topic #0 (0.250): 0.004*\"joke\" + 0.004*\"mom\" + 0.003*\"parents\" + 0.003*\"guns\" + 0.003*\"ass\" + 0.003*\"wife\" + 0.002*\"friend\" + 0.002*\"clinton\" + 0.002*\"gun\" + 0.002*\"son\"\n",
      "2020-01-30 20:30:34,943 : INFO : topic #1 (0.250): 0.003*\"joke\" + 0.003*\"mom\" + 0.002*\"comedy\" + 0.002*\"jokes\" + 0.002*\"parents\" + 0.002*\"love\" + 0.002*\"door\" + 0.002*\"friend\" + 0.002*\"young\" + 0.002*\"dick\"\n",
      "2020-01-30 20:30:34,949 : INFO : topic #2 (0.250): 0.003*\"mom\" + 0.003*\"parents\" + 0.003*\"door\" + 0.002*\"joke\" + 0.002*\"dick\" + 0.002*\"son\" + 0.002*\"dog\" + 0.002*\"friend\" + 0.002*\"love\" + 0.002*\"um\"\n",
      "2020-01-30 20:30:34,955 : INFO : topic #3 (0.250): 0.003*\"mom\" + 0.003*\"joke\" + 0.003*\"ass\" + 0.003*\"parents\" + 0.002*\"ok\" + 0.002*\"dead\" + 0.002*\"comedy\" + 0.002*\"jokes\" + 0.002*\"door\" + 0.002*\"mad\"\n",
      "2020-01-30 20:30:34,960 : INFO : topic diff=1.199275, rho=1.000000\n",
      "2020-01-30 20:30:35,540 : INFO : -8.887 per-word bound, 473.4 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:35,540 : INFO : PROGRESS: pass 1, at document #12/12\n",
      "2020-01-30 20:30:35,666 : INFO : topic #0 (0.250): 0.006*\"joke\" + 0.004*\"mom\" + 0.003*\"guns\" + 0.003*\"clinton\" + 0.003*\"wife\" + 0.003*\"parents\" + 0.003*\"ass\" + 0.003*\"friend\" + 0.003*\"anthony\" + 0.003*\"son\"\n",
      "2020-01-30 20:30:35,670 : INFO : topic #1 (0.250): 0.003*\"gun\" + 0.003*\"gay\" + 0.003*\"jesus\" + 0.002*\"dick\" + 0.002*\"joke\" + 0.002*\"young\" + 0.002*\"ahah\" + 0.002*\"sense\" + 0.002*\"religion\" + 0.002*\"son\"\n",
      "2020-01-30 20:30:35,674 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"mom\" + 0.003*\"hasan\" + 0.003*\"door\" + 0.003*\"jenny\" + 0.003*\"love\" + 0.003*\"bo\" + 0.003*\"comedy\" + 0.002*\"eye\" + 0.002*\"brown\"\n",
      "2020-01-30 20:30:35,678 : INFO : topic #3 (0.250): 0.004*\"ok\" + 0.003*\"joke\" + 0.003*\"husband\" + 0.003*\"mom\" + 0.003*\"ass\" + 0.003*\"pregnant\" + 0.002*\"doctor\" + 0.002*\"dead\" + 0.002*\"jenner\" + 0.002*\"nuts\"\n",
      "2020-01-30 20:30:35,682 : INFO : topic diff=0.649345, rho=0.577350\n",
      "2020-01-30 20:30:36,201 : INFO : -8.519 per-word bound, 366.8 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:36,205 : INFO : PROGRESS: pass 2, at document #12/12\n",
      "2020-01-30 20:30:36,346 : INFO : topic #0 (0.250): 0.006*\"joke\" + 0.005*\"mom\" + 0.004*\"clinton\" + 0.004*\"guns\" + 0.004*\"wife\" + 0.004*\"ass\" + 0.004*\"anthony\" + 0.004*\"friend\" + 0.003*\"parents\" + 0.003*\"son\"\n",
      "2020-01-30 20:30:36,350 : INFO : topic #1 (0.250): 0.003*\"ahah\" + 0.003*\"gay\" + 0.003*\"gun\" + 0.003*\"jesus\" + 0.003*\"son\" + 0.003*\"dick\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"religion\" + 0.003*\"sense\"\n",
      "2020-01-30 20:30:36,354 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.003*\"bo\" + 0.003*\"jenny\" + 0.003*\"comedy\" + 0.003*\"door\" + 0.003*\"love\" + 0.003*\"repeat\" + 0.003*\"eye\"\n",
      "2020-01-30 20:30:36,358 : INFO : topic #3 (0.250): 0.005*\"joke\" + 0.004*\"husband\" + 0.004*\"ok\" + 0.003*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"jenner\" + 0.003*\"nuts\" + 0.003*\"dead\" + 0.003*\"ass\" + 0.003*\"mom\"\n",
      "2020-01-30 20:30:36,361 : INFO : topic diff=0.449530, rho=0.500000\n",
      "2020-01-30 20:30:36,842 : INFO : -8.364 per-word bound, 329.6 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:36,843 : INFO : PROGRESS: pass 3, at document #12/12\n",
      "2020-01-30 20:30:36,902 : INFO : topic #0 (0.250): 0.006*\"joke\" + 0.005*\"mom\" + 0.005*\"clinton\" + 0.004*\"guns\" + 0.004*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"friend\" + 0.003*\"parents\" + 0.003*\"gun\"\n",
      "2020-01-30 20:30:36,905 : INFO : topic #1 (0.250): 0.004*\"ahah\" + 0.004*\"gay\" + 0.003*\"gun\" + 0.003*\"son\" + 0.003*\"jesus\" + 0.003*\"wife\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"dick\" + 0.003*\"trouble\"\n",
      "2020-01-30 20:30:36,909 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"mom\" + 0.004*\"bo\" + 0.004*\"hasan\" + 0.003*\"jenny\" + 0.003*\"comedy\" + 0.003*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:36,912 : INFO : topic #3 (0.250): 0.006*\"joke\" + 0.004*\"husband\" + 0.004*\"ok\" + 0.003*\"jenner\" + 0.003*\"nuts\" + 0.003*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"dead\"\n",
      "2020-01-30 20:30:36,914 : INFO : topic diff=0.291235, rho=0.447214\n",
      "2020-01-30 20:30:37,270 : INFO : -8.298 per-word bound, 314.8 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:37,273 : INFO : PROGRESS: pass 4, at document #12/12\n",
      "2020-01-30 20:30:37,339 : INFO : topic #0 (0.250): 0.006*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.004*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"friend\" + 0.003*\"gun\" + 0.003*\"parents\"\n",
      "2020-01-30 20:30:37,342 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"gun\" + 0.004*\"son\" + 0.003*\"wife\" + 0.003*\"nigga\" + 0.003*\"jesus\" + 0.003*\"young\" + 0.003*\"trouble\" + 0.003*\"dick\"\n",
      "2020-01-30 20:30:37,347 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.003*\"jenny\" + 0.003*\"comedy\" + 0.003*\"repeat\" + 0.003*\"eye\" + 0.003*\"door\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:37,352 : INFO : topic #3 (0.250): 0.006*\"joke\" + 0.004*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.003*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:37,357 : INFO : topic diff=0.187467, rho=0.408248\n",
      "2020-01-30 20:30:37,705 : INFO : -8.271 per-word bound, 308.9 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:37,707 : INFO : PROGRESS: pass 5, at document #12/12\n",
      "2020-01-30 20:30:37,748 : INFO : topic #0 (0.250): 0.006*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.004*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.003*\"friend\" + 0.003*\"cow\"\n",
      "2020-01-30 20:30:37,751 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"son\" + 0.004*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"young\" + 0.003*\"trouble\" + 0.003*\"somebody\"\n",
      "2020-01-30 20:30:37,754 : INFO : topic #2 (0.250): 0.005*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.003*\"comedy\" + 0.003*\"repeat\" + 0.003*\"eye\" + 0.003*\"door\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:37,758 : INFO : topic #3 (0.250): 0.006*\"joke\" + 0.004*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:37,761 : INFO : topic diff=0.124578, rho=0.377964\n",
      "2020-01-30 20:30:38,072 : INFO : -8.257 per-word bound, 306.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:38,074 : INFO : PROGRESS: pass 6, at document #12/12\n",
      "2020-01-30 20:30:38,118 : INFO : topic #0 (0.250): 0.006*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.004*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:38,121 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"son\" + 0.004*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"young\" + 0.003*\"tit\" + 0.003*\"trouble\"\n",
      "2020-01-30 20:30:38,123 : INFO : topic #2 (0.250): 0.005*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.003*\"comedy\" + 0.003*\"repeat\" + 0.003*\"eye\" + 0.003*\"door\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:38,127 : INFO : topic #3 (0.250): 0.006*\"joke\" + 0.004*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:38,130 : INFO : topic diff=0.085594, rho=0.353553\n",
      "2020-01-30 20:30:38,469 : INFO : -8.249 per-word bound, 304.2 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:30:38,471 : INFO : PROGRESS: pass 7, at document #12/12\n",
      "2020-01-30 20:30:38,510 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.004*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:38,512 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"son\" + 0.004*\"gun\" + 0.004*\"nigga\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"young\" + 0.003*\"tit\" + 0.003*\"trouble\"\n",
      "2020-01-30 20:30:38,514 : INFO : topic #2 (0.250): 0.005*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.003*\"comedy\" + 0.003*\"repeat\" + 0.003*\"eye\" + 0.003*\"door\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:38,517 : INFO : topic #3 (0.250): 0.006*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:38,519 : INFO : topic diff=0.060453, rho=0.333333\n",
      "2020-01-30 20:30:38,806 : INFO : -8.243 per-word bound, 303.1 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:38,808 : INFO : PROGRESS: pass 8, at document #12/12\n",
      "2020-01-30 20:30:38,844 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.004*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:38,846 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"son\" + 0.004*\"gun\" + 0.004*\"nigga\" + 0.003*\"wife\" + 0.003*\"tit\" + 0.003*\"jesus\" + 0.003*\"young\" + 0.003*\"trouble\"\n",
      "2020-01-30 20:30:38,848 : INFO : topic #2 (0.250): 0.005*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.003*\"comedy\" + 0.003*\"repeat\" + 0.003*\"eye\" + 0.003*\"door\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:38,849 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:38,854 : INFO : topic diff=0.043381, rho=0.316228\n",
      "2020-01-30 20:30:39,114 : INFO : -8.240 per-word bound, 302.3 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:39,115 : INFO : PROGRESS: pass 9, at document #12/12\n",
      "2020-01-30 20:30:39,157 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:39,158 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"son\" + 0.004*\"gun\" + 0.004*\"nigga\" + 0.003*\"tit\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"trouble\"\n",
      "2020-01-30 20:30:39,160 : INFO : topic #2 (0.250): 0.005*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.003*\"comedy\" + 0.003*\"repeat\" + 0.003*\"eye\" + 0.003*\"door\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:39,164 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:39,166 : INFO : topic diff=0.031615, rho=0.301511\n",
      "2020-01-30 20:30:39,440 : INFO : -8.237 per-word bound, 301.7 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:39,441 : INFO : PROGRESS: pass 10, at document #12/12\n",
      "2020-01-30 20:30:39,480 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:39,483 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"son\" + 0.004*\"tit\" + 0.004*\"gun\" + 0.004*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"trouble\"\n",
      "2020-01-30 20:30:39,486 : INFO : topic #2 (0.250): 0.005*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.003*\"comedy\" + 0.003*\"repeat\" + 0.003*\"eye\" + 0.003*\"door\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:39,489 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:39,491 : INFO : topic diff=0.023405, rho=0.288675\n",
      "2020-01-30 20:30:39,804 : INFO : -8.235 per-word bound, 301.3 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:39,806 : INFO : PROGRESS: pass 11, at document #12/12\n",
      "2020-01-30 20:30:39,842 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:39,845 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"son\" + 0.004*\"tit\" + 0.004*\"gun\" + 0.004*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:39,847 : INFO : topic #2 (0.250): 0.005*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.003*\"comedy\" + 0.003*\"repeat\" + 0.003*\"eye\" + 0.003*\"door\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:39,850 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:39,853 : INFO : topic diff=0.017582, rho=0.277350\n",
      "2020-01-30 20:30:40,115 : INFO : -8.234 per-word bound, 301.1 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:40,117 : INFO : PROGRESS: pass 12, at document #12/12\n",
      "2020-01-30 20:30:40,151 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:40,154 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"son\" + 0.004*\"tit\" + 0.004*\"gun\" + 0.004*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:40,157 : INFO : topic #2 (0.250): 0.005*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.003*\"comedy\" + 0.003*\"repeat\" + 0.003*\"eye\" + 0.003*\"door\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:40,159 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:40,162 : INFO : topic diff=0.013384, rho=0.267261\n",
      "2020-01-30 20:30:40,444 : INFO : -8.233 per-word bound, 300.9 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:40,445 : INFO : PROGRESS: pass 13, at document #12/12\n",
      "2020-01-30 20:30:40,476 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:40,478 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.004*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:40,481 : INFO : topic #2 (0.250): 0.005*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.003*\"comedy\" + 0.003*\"repeat\" + 0.003*\"eye\" + 0.003*\"door\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:40,484 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:40,486 : INFO : topic diff=0.010327, rho=0.258199\n",
      "2020-01-30 20:30:40,729 : INFO : -8.232 per-word bound, 300.7 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:40,731 : INFO : PROGRESS: pass 14, at document #12/12\n",
      "2020-01-30 20:30:40,760 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:40,762 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:40,764 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.003*\"comedy\" + 0.003*\"repeat\" + 0.003*\"eye\" + 0.003*\"door\" + 0.003*\"love\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:30:40,767 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:40,770 : INFO : topic diff=0.008088, rho=0.250000\n",
      "2020-01-30 20:30:41,032 : INFO : -8.232 per-word bound, 300.6 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:41,034 : INFO : PROGRESS: pass 15, at document #12/12\n",
      "2020-01-30 20:30:41,067 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:41,070 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:41,072 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.003*\"comedy\" + 0.003*\"repeat\" + 0.003*\"eye\" + 0.003*\"door\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:41,075 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:41,078 : INFO : topic diff=0.006437, rho=0.242536\n",
      "2020-01-30 20:30:41,319 : INFO : -8.231 per-word bound, 300.5 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:41,321 : INFO : PROGRESS: pass 16, at document #12/12\n",
      "2020-01-30 20:30:41,352 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:41,354 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:41,356 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"eye\" + 0.003*\"door\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:41,359 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:41,361 : INFO : topic diff=0.005201, rho=0.235702\n",
      "2020-01-30 20:30:41,596 : INFO : -8.231 per-word bound, 300.4 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:41,597 : INFO : PROGRESS: pass 17, at document #12/12\n",
      "2020-01-30 20:30:41,627 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:41,630 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:41,632 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"eye\" + 0.003*\"door\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:41,635 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:41,637 : INFO : topic diff=0.004256, rho=0.229416\n",
      "2020-01-30 20:30:41,878 : INFO : -8.231 per-word bound, 300.4 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:41,880 : INFO : PROGRESS: pass 18, at document #12/12\n",
      "2020-01-30 20:30:41,909 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:41,912 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:41,914 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"eye\" + 0.003*\"door\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:41,917 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:41,919 : INFO : topic diff=0.003514, rho=0.223607\n",
      "2020-01-30 20:30:42,172 : INFO : -8.230 per-word bound, 300.3 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:42,173 : INFO : PROGRESS: pass 19, at document #12/12\n",
      "2020-01-30 20:30:42,212 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:42,214 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:42,215 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"eye\" + 0.003*\"door\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:42,217 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:42,220 : INFO : topic diff=0.002909, rho=0.218218\n",
      "2020-01-30 20:30:42,478 : INFO : -8.230 per-word bound, 300.2 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:42,480 : INFO : PROGRESS: pass 20, at document #12/12\n",
      "2020-01-30 20:30:42,515 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:42,517 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:42,520 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"eye\" + 0.003*\"door\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:42,523 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:42,525 : INFO : topic diff=0.002411, rho=0.213201\n",
      "2020-01-30 20:30:42,757 : INFO : -8.230 per-word bound, 300.2 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:42,758 : INFO : PROGRESS: pass 21, at document #12/12\n",
      "2020-01-30 20:30:42,788 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:42,790 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:42,792 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"eye\" + 0.003*\"door\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:42,795 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:42,796 : INFO : topic diff=0.002014, rho=0.208514\n",
      "2020-01-30 20:30:43,024 : INFO : -8.229 per-word bound, 300.1 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:43,026 : INFO : PROGRESS: pass 22, at document #12/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:30:43,055 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:43,057 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:43,059 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"eye\" + 0.003*\"door\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:43,061 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:43,064 : INFO : topic diff=0.001733, rho=0.204124\n",
      "2020-01-30 20:30:43,289 : INFO : -8.229 per-word bound, 300.1 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:43,290 : INFO : PROGRESS: pass 23, at document #12/12\n",
      "2020-01-30 20:30:43,316 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:43,319 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:43,321 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:43,323 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:43,325 : INFO : topic diff=0.001310, rho=0.200000\n",
      "2020-01-30 20:30:43,554 : INFO : -8.229 per-word bound, 300.1 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:43,556 : INFO : PROGRESS: pass 24, at document #12/12\n",
      "2020-01-30 20:30:43,582 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:43,584 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:43,586 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:43,589 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:43,591 : INFO : topic diff=0.001015, rho=0.196116\n",
      "2020-01-30 20:30:43,842 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:43,844 : INFO : PROGRESS: pass 25, at document #12/12\n",
      "2020-01-30 20:30:43,884 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:43,887 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:43,890 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:43,892 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:43,895 : INFO : topic diff=0.000795, rho=0.192450\n",
      "2020-01-30 20:30:44,154 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:44,156 : INFO : PROGRESS: pass 26, at document #12/12\n",
      "2020-01-30 20:30:44,184 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:44,186 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:44,188 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:44,191 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"jenner\" + 0.004*\"nuts\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:44,193 : INFO : topic diff=0.000626, rho=0.188982\n",
      "2020-01-30 20:30:44,443 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:44,445 : INFO : PROGRESS: pass 27, at document #12/12\n",
      "2020-01-30 20:30:44,482 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:44,484 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:44,486 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:44,489 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:44,491 : INFO : topic diff=0.000497, rho=0.185695\n",
      "2020-01-30 20:30:44,768 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:44,770 : INFO : PROGRESS: pass 28, at document #12/12\n",
      "2020-01-30 20:30:44,798 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:44,801 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:44,803 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:44,806 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:44,808 : INFO : topic diff=0.000397, rho=0.182574\n",
      "2020-01-30 20:30:44,989 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:44,991 : INFO : PROGRESS: pass 29, at document #12/12\n",
      "2020-01-30 20:30:45,014 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:45,016 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:45,018 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:30:45,020 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:45,022 : INFO : topic diff=0.000318, rho=0.179605\n",
      "2020-01-30 20:30:45,205 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:45,206 : INFO : PROGRESS: pass 30, at document #12/12\n",
      "2020-01-30 20:30:45,228 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:45,229 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:45,231 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:45,233 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:45,235 : INFO : topic diff=0.000257, rho=0.176777\n",
      "2020-01-30 20:30:45,396 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:45,397 : INFO : PROGRESS: pass 31, at document #12/12\n",
      "2020-01-30 20:30:45,419 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:45,421 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:45,423 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:45,425 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:45,427 : INFO : topic diff=0.000209, rho=0.174078\n",
      "2020-01-30 20:30:45,626 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:45,628 : INFO : PROGRESS: pass 32, at document #12/12\n",
      "2020-01-30 20:30:45,653 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:45,655 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:45,657 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:45,659 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:45,661 : INFO : topic diff=0.000169, rho=0.171499\n",
      "2020-01-30 20:30:45,857 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:45,859 : INFO : PROGRESS: pass 33, at document #12/12\n",
      "2020-01-30 20:30:45,887 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:45,889 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:45,891 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:45,894 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:45,895 : INFO : topic diff=0.000139, rho=0.169031\n",
      "2020-01-30 20:30:46,083 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:46,084 : INFO : PROGRESS: pass 34, at document #12/12\n",
      "2020-01-30 20:30:46,105 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:46,107 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:46,108 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:46,110 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:46,112 : INFO : topic diff=0.000114, rho=0.166667\n",
      "2020-01-30 20:30:46,356 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:46,360 : INFO : PROGRESS: pass 35, at document #12/12\n",
      "2020-01-30 20:30:46,393 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:46,395 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:46,397 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:46,400 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:46,402 : INFO : topic diff=0.000094, rho=0.164399\n",
      "2020-01-30 20:30:46,579 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:46,580 : INFO : PROGRESS: pass 36, at document #12/12\n",
      "2020-01-30 20:30:46,605 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:46,608 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:46,611 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:46,614 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:46,617 : INFO : topic diff=0.000078, rho=0.162221\n",
      "2020-01-30 20:30:46,814 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:46,815 : INFO : PROGRESS: pass 37, at document #12/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:30:46,841 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:46,844 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:46,846 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:46,848 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:46,852 : INFO : topic diff=0.000065, rho=0.160128\n",
      "2020-01-30 20:30:47,082 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:47,084 : INFO : PROGRESS: pass 38, at document #12/12\n",
      "2020-01-30 20:30:47,114 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:47,118 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:47,121 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:47,123 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:47,126 : INFO : topic diff=0.000054, rho=0.158114\n",
      "2020-01-30 20:30:47,327 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:47,329 : INFO : PROGRESS: pass 39, at document #12/12\n",
      "2020-01-30 20:30:47,350 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:47,352 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:47,355 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:47,358 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:47,361 : INFO : topic diff=0.000044, rho=0.156174\n",
      "2020-01-30 20:30:47,539 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:47,540 : INFO : PROGRESS: pass 40, at document #12/12\n",
      "2020-01-30 20:30:47,566 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:47,568 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:47,570 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:47,572 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:47,574 : INFO : topic diff=0.000037, rho=0.154303\n",
      "2020-01-30 20:30:47,753 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:47,754 : INFO : PROGRESS: pass 41, at document #12/12\n",
      "2020-01-30 20:30:47,776 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:47,778 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:47,781 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:47,783 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:47,785 : INFO : topic diff=0.000031, rho=0.152499\n",
      "2020-01-30 20:30:47,970 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:47,971 : INFO : PROGRESS: pass 42, at document #12/12\n",
      "2020-01-30 20:30:47,992 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:47,993 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:47,996 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:48,000 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:48,003 : INFO : topic diff=0.000026, rho=0.150756\n",
      "2020-01-30 20:30:48,181 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:48,181 : INFO : PROGRESS: pass 43, at document #12/12\n",
      "2020-01-30 20:30:48,210 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:48,213 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:48,214 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:48,216 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:48,218 : INFO : topic diff=0.000022, rho=0.149071\n",
      "2020-01-30 20:30:48,450 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:48,452 : INFO : PROGRESS: pass 44, at document #12/12\n",
      "2020-01-30 20:30:48,486 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:48,488 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:48,490 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:30:48,492 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:48,494 : INFO : topic diff=0.000018, rho=0.147442\n",
      "2020-01-30 20:30:48,717 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:48,719 : INFO : PROGRESS: pass 45, at document #12/12\n",
      "2020-01-30 20:30:48,760 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:48,762 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:48,764 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:48,766 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:48,768 : INFO : topic diff=0.000015, rho=0.145865\n",
      "2020-01-30 20:30:49,075 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:49,076 : INFO : PROGRESS: pass 46, at document #12/12\n",
      "2020-01-30 20:30:49,106 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:49,109 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:49,112 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:49,115 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:49,116 : INFO : topic diff=0.000013, rho=0.144338\n",
      "2020-01-30 20:30:49,304 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:49,305 : INFO : PROGRESS: pass 47, at document #12/12\n",
      "2020-01-30 20:30:49,330 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:49,332 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:49,334 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:49,337 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:49,339 : INFO : topic diff=0.000011, rho=0.142857\n",
      "2020-01-30 20:30:49,519 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:49,520 : INFO : PROGRESS: pass 48, at document #12/12\n",
      "2020-01-30 20:30:49,545 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:49,547 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:49,549 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:49,551 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:49,553 : INFO : topic diff=0.000010, rho=0.141421\n",
      "2020-01-30 20:30:49,726 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:49,728 : INFO : PROGRESS: pass 49, at document #12/12\n",
      "2020-01-30 20:30:49,754 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:49,756 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:49,759 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:49,762 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:49,764 : INFO : topic diff=0.000008, rho=0.140028\n",
      "2020-01-30 20:30:49,954 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:49,955 : INFO : PROGRESS: pass 50, at document #12/12\n",
      "2020-01-30 20:30:49,980 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:49,982 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:49,983 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:49,986 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:49,988 : INFO : topic diff=0.000007, rho=0.138675\n",
      "2020-01-30 20:30:50,173 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:50,174 : INFO : PROGRESS: pass 51, at document #12/12\n",
      "2020-01-30 20:30:50,197 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:50,200 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:50,202 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:50,204 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:50,207 : INFO : topic diff=0.000006, rho=0.137361\n",
      "2020-01-30 20:30:50,379 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:50,380 : INFO : PROGRESS: pass 52, at document #12/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:30:50,410 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:50,412 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:50,414 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:50,416 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:50,417 : INFO : topic diff=0.000005, rho=0.136083\n",
      "2020-01-30 20:30:50,593 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:50,594 : INFO : PROGRESS: pass 53, at document #12/12\n",
      "2020-01-30 20:30:50,623 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:50,625 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:50,626 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:50,628 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:50,630 : INFO : topic diff=0.000004, rho=0.134840\n",
      "2020-01-30 20:30:50,812 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:50,813 : INFO : PROGRESS: pass 54, at document #12/12\n",
      "2020-01-30 20:30:50,841 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:50,843 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:50,844 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:50,846 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:50,848 : INFO : topic diff=0.000004, rho=0.133631\n",
      "2020-01-30 20:30:51,017 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:51,019 : INFO : PROGRESS: pass 55, at document #12/12\n",
      "2020-01-30 20:30:51,046 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:51,048 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:51,051 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:51,053 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:51,055 : INFO : topic diff=0.000003, rho=0.132453\n",
      "2020-01-30 20:30:51,229 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:51,231 : INFO : PROGRESS: pass 56, at document #12/12\n",
      "2020-01-30 20:30:51,258 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:51,260 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:51,262 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:51,264 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:51,266 : INFO : topic diff=0.000003, rho=0.131306\n",
      "2020-01-30 20:30:51,453 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:51,455 : INFO : PROGRESS: pass 57, at document #12/12\n",
      "2020-01-30 20:30:51,487 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:51,490 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:51,492 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:51,496 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:51,498 : INFO : topic diff=0.000002, rho=0.130189\n",
      "2020-01-30 20:30:51,727 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:51,728 : INFO : PROGRESS: pass 58, at document #12/12\n",
      "2020-01-30 20:30:51,754 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:51,757 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:51,759 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:51,765 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:51,768 : INFO : topic diff=0.000002, rho=0.129099\n",
      "2020-01-30 20:30:51,988 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:51,990 : INFO : PROGRESS: pass 59, at document #12/12\n",
      "2020-01-30 20:30:52,021 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:52,024 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"wife\" + 0.003*\"young\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:52,027 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:30:52,030 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"hampstead\" + 0.003*\"chimp\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:52,032 : INFO : topic diff=0.000002, rho=0.128037\n",
      "2020-01-30 20:30:52,221 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:52,223 : INFO : PROGRESS: pass 60, at document #12/12\n",
      "2020-01-30 20:30:52,252 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:52,258 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:52,261 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:52,263 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"hampstead\" + 0.003*\"chimp\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:52,265 : INFO : topic diff=0.000002, rho=0.127000\n",
      "2020-01-30 20:30:52,477 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:52,479 : INFO : PROGRESS: pass 61, at document #12/12\n",
      "2020-01-30 20:30:52,509 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:52,514 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:52,516 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:52,519 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:52,521 : INFO : topic diff=0.000001, rho=0.125988\n",
      "2020-01-30 20:30:52,723 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:52,724 : INFO : PROGRESS: pass 62, at document #12/12\n",
      "2020-01-30 20:30:52,751 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:52,753 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:52,756 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:52,758 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:52,760 : INFO : topic diff=0.000001, rho=0.125000\n",
      "2020-01-30 20:30:52,938 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:52,939 : INFO : PROGRESS: pass 63, at document #12/12\n",
      "2020-01-30 20:30:52,967 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:52,971 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:52,974 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:52,976 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:52,980 : INFO : topic diff=0.000001, rho=0.124035\n",
      "2020-01-30 20:30:53,162 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:53,163 : INFO : PROGRESS: pass 64, at document #12/12\n",
      "2020-01-30 20:30:53,195 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:53,201 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:53,204 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:53,206 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:53,209 : INFO : topic diff=0.000001, rho=0.123091\n",
      "2020-01-30 20:30:53,403 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:53,404 : INFO : PROGRESS: pass 65, at document #12/12\n",
      "2020-01-30 20:30:53,431 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:53,433 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:53,436 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:53,438 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:53,443 : INFO : topic diff=0.000001, rho=0.122169\n",
      "2020-01-30 20:30:53,629 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:53,630 : INFO : PROGRESS: pass 66, at document #12/12\n",
      "2020-01-30 20:30:53,662 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:53,664 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:53,666 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:53,669 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"hampstead\" + 0.003*\"chimp\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:53,671 : INFO : topic diff=0.000001, rho=0.121268\n",
      "2020-01-30 20:30:53,913 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:53,914 : INFO : PROGRESS: pass 67, at document #12/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:30:53,939 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:53,941 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:53,943 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:53,946 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:53,948 : INFO : topic diff=0.000001, rho=0.120386\n",
      "2020-01-30 20:30:54,136 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:54,137 : INFO : PROGRESS: pass 68, at document #12/12\n",
      "2020-01-30 20:30:54,166 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:54,167 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:54,169 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:54,171 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"hampstead\" + 0.003*\"chimp\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:54,173 : INFO : topic diff=0.000000, rho=0.119523\n",
      "2020-01-30 20:30:54,357 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:54,358 : INFO : PROGRESS: pass 69, at document #12/12\n",
      "2020-01-30 20:30:54,381 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:54,383 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:54,385 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:54,386 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:54,388 : INFO : topic diff=0.000000, rho=0.118678\n",
      "2020-01-30 20:30:54,558 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:54,560 : INFO : PROGRESS: pass 70, at document #12/12\n",
      "2020-01-30 20:30:54,586 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:54,587 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:54,589 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:54,593 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:54,595 : INFO : topic diff=0.000001, rho=0.117851\n",
      "2020-01-30 20:30:54,773 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:54,774 : INFO : PROGRESS: pass 71, at document #12/12\n",
      "2020-01-30 20:30:54,797 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:54,799 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:54,802 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:54,804 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"hampstead\" + 0.003*\"chimp\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:54,806 : INFO : topic diff=0.000000, rho=0.117041\n",
      "2020-01-30 20:30:54,981 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:54,982 : INFO : PROGRESS: pass 72, at document #12/12\n",
      "2020-01-30 20:30:55,010 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:55,013 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:55,015 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:55,017 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"hampstead\" + 0.003*\"chimp\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:55,019 : INFO : topic diff=0.000000, rho=0.116248\n",
      "2020-01-30 20:30:55,205 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:55,207 : INFO : PROGRESS: pass 73, at document #12/12\n",
      "2020-01-30 20:30:55,234 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:55,236 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:55,238 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:55,241 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:55,243 : INFO : topic diff=0.000000, rho=0.115470\n",
      "2020-01-30 20:30:55,437 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:55,438 : INFO : PROGRESS: pass 74, at document #12/12\n",
      "2020-01-30 20:30:55,465 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:55,468 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:55,469 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 20:30:55,473 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"hampstead\" + 0.003*\"chimp\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:55,475 : INFO : topic diff=0.000000, rho=0.114708\n",
      "2020-01-30 20:30:55,644 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:55,646 : INFO : PROGRESS: pass 75, at document #12/12\n",
      "2020-01-30 20:30:55,673 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:55,675 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:55,677 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:55,679 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:55,681 : INFO : topic diff=0.000000, rho=0.113961\n",
      "2020-01-30 20:30:55,880 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:55,882 : INFO : PROGRESS: pass 76, at document #12/12\n",
      "2020-01-30 20:30:55,911 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:55,913 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:55,915 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:55,917 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"chimp\" + 0.003*\"hampstead\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:55,919 : INFO : topic diff=0.000000, rho=0.113228\n",
      "2020-01-30 20:30:56,086 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:56,088 : INFO : PROGRESS: pass 77, at document #12/12\n",
      "2020-01-30 20:30:56,113 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:56,115 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:56,119 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:56,122 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"hampstead\" + 0.003*\"chimp\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:56,124 : INFO : topic diff=0.000000, rho=0.112509\n",
      "2020-01-30 20:30:56,301 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:56,303 : INFO : PROGRESS: pass 78, at document #12/12\n",
      "2020-01-30 20:30:56,333 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:56,335 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:56,337 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:56,340 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"hampstead\" + 0.003*\"chimp\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:56,342 : INFO : topic diff=0.000000, rho=0.111803\n",
      "2020-01-30 20:30:56,530 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 12 documents with 17697 words\n",
      "2020-01-30 20:30:56,531 : INFO : PROGRESS: pass 79, at document #12/12\n",
      "2020-01-30 20:30:56,556 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:56,558 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:56,560 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:56,564 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"hampstead\" + 0.003*\"chimp\" + 0.003*\"rape\"\n",
      "2020-01-30 20:30:56,566 : INFO : topic diff=0.000000, rho=0.111111\n",
      "2020-01-30 20:30:56,571 : INFO : topic #0 (0.250): 0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"\n",
      "2020-01-30 20:30:56,574 : INFO : topic #1 (0.250): 0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"\n",
      "2020-01-30 20:30:56,576 : INFO : topic #2 (0.250): 0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"\n",
      "2020-01-30 20:30:56,578 : INFO : topic #3 (0.250): 0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"hampstead\" + 0.003*\"chimp\" + 0.003*\"rape\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"joke\" + 0.006*\"mom\" + 0.005*\"clinton\" + 0.005*\"guns\" + 0.005*\"anthony\" + 0.004*\"ass\" + 0.004*\"wife\" + 0.004*\"gun\" + 0.004*\"cow\" + 0.003*\"friend\"'),\n",
       " (1,\n",
       "  '0.005*\"ahah\" + 0.004*\"gay\" + 0.004*\"tit\" + 0.004*\"son\" + 0.003*\"gun\" + 0.003*\"nigga\" + 0.003*\"young\" + 0.003*\"wife\" + 0.003*\"jesus\" + 0.003*\"older\"'),\n",
       " (2,\n",
       "  '0.004*\"parents\" + 0.004*\"bo\" + 0.004*\"mom\" + 0.004*\"hasan\" + 0.004*\"jenny\" + 0.004*\"comedy\" + 0.004*\"repeat\" + 0.003*\"door\" + 0.003*\"eye\" + 0.003*\"love\"'),\n",
       " (3,\n",
       "  '0.007*\"joke\" + 0.005*\"husband\" + 0.004*\"ok\" + 0.004*\"nuts\" + 0.004*\"jenner\" + 0.004*\"doctor\" + 0.003*\"pregnant\" + 0.003*\"hampstead\" + 0.003*\"chimp\" + 0.003*\"rape\"')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four topics look pretty decent. Let's settle on these for now.\n",
    "\n",
    "Topic 0: mom, parents\n",
    "\n",
    "Topic 1: husband, wife\n",
    "\n",
    "Topic 2: guns\n",
    "\n",
    "Topic 3: profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'ali'),\n",
       " (0, 'anthony'),\n",
       " (1, 'bill'),\n",
       " (2, 'bo'),\n",
       " (1, 'dave'),\n",
       " (2, 'hasan'),\n",
       " (0, 'jim'),\n",
       " (2, 'joe'),\n",
       " (0, 'john'),\n",
       " (1, 'louis'),\n",
       " (2, 'mike'),\n",
       " (3, 'ricky')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a first pass of LDA, these kind of make sense to me, so we'll call it a day for now.\n",
    "\n",
    "Topic 0: mom, parents [Anthony, jim, john]\n",
    "    \n",
    "Topic 1: husband, wife [bill, dave, louis]\n",
    "    \n",
    "Topic 2: guns [bo, hasan, joe]\n",
    "    \n",
    "Topic 3: profanity [ali, ricky]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
